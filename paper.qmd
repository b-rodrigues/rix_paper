---
title: "Nix for Polyglot, Reproducible Data Science Workflows"
format:
    jss-pdf:
        keep-tex: true
    jss-html:
        embed-resources: true
        toc: true
author:
  - name: Bruno Rodrigues
    affiliations:
      - name: Ministry of Research and Higher education, Luxembourg
        department: Department of Statistics
        address: 18, Montée de la Pétrusse
        city: Luxembourg
        country: Luxembourg
        postal-code: 2327
      - Journal of Statistical Software
    orcid: 0000-0002-3211-3689
    email: bruno@brodrigues.co
    url: https://www.brodrigues.co
  - name: Philipp Baumann
    affiliations:
      - name: Alliance SwissPass, Switzerland
        department: Data and Analytics
        address: 7, Länggassstrasse
        city: Bern
        country: Switzerland
        postal-code: 3012
    orcid: 0000-0002-3194-8975
    email: baumann-philipp@protonmail.com
abstract: |
  Reproducible analysis requires more than clean, well-documented code; it
  demands the precise management of software dependencies, computational
  environments, and workflow execution. While researchers can combine tools like
  [Docker]{.pkg}, [renv]{.pkg} for [R]{.proglang} packages (or [uv]{.pkg} for
  [Python]{.proglang} packages) and [Make]{.pkg} to address these needs, the
  resulting toolchain is often complex and fragile, requiring the coordination
  of multiple disparate systems. This paper introduces a unified framework built
  on the [Nix]{.pkg} package manager, made accessible through two [R]{.proglang}
  packages. The first, [rix]{.pkg}, generates declarative [Nix]{.pkg}
  expressions to define version-pinned, reproducible environments that encompass
  [R]{.proglang}, [Python]{.proglang}, [Julia]{.proglang}, and system-level
  dependencies. The second, [rixpress]{.pkg} (and its [Python]{.proglang} port,
  [ryxpress]{.pkg}), leverages these environments to orchestrate polyglot
  pipelines where each computational step runs in a hermetically sealed,
  language-specific environment with automatic caching. By treating the entire
  computational environment as code, this integrated approach ensures a high
  degree of reproducibility, supports collaboration across heterogeneous
  systems, and helps guarantee that analyses remain executable long into the
  future.

keywords: [reproducibility, R, Nix]
keywords-formatted: [reproducibility, "[R]{.proglang}", "[Python]{.proglang}", "[Julia]{.proglang}", "[Nix]{.pkg}"]
number-sections: true

bibliography: bibliography.bib
---

## Introduction: Reproducibility is also about software {#sec-intro}

@peng2011 introduced the concept of reproducibility as a *continuum*. At one end
lies the least reproducible state, where only a paper describing the study is
available. Reproducibility improves when authors share the original source code,
improves further when they include the underlying data, and reaches its highest
level when what Roger Peng called *linked and executable code and data* are
provided.

By *linked and executable code and data*, Peng referred to compiled source code
and runnable scripts. In this paper, we interpret this notion more broadly as the
*computational environment*: the complete set of software required to execute an
analysis. Here too, a continuum exists. At the minimal end, authors might only
name the main software used—say, the [R]{.proglang} programming language
[@r-core-team]. More careful authors might also specify the version of
[R]{.proglang}, or list the additional packages and their versions. Rarely,
however, do authors specify the operating system on which the analysis was
performed, even though differences in operating systems can lead to divergent
results when using the same code and software versions, as shown by
@neupane2019. It is even less common for authors to provide step-by-step
installation instructions for the required software stack, an omission often
driven by institutional constraints. Journals, for instance, can inadvertently
hinder reproducibility by imposing strict page or word limits that leave no room
for the necessary technical documentation, discouraging thoroughness in the name
of brevity.

Even when such instructions are given, they often fail across different
platforms or versions of the same platform. This lack of portability not only
hinders reproducibility but also complicates everyday research workflows.
Researchers using multiple machines must recreate environments consistently, and
collaborators must share identical computational setups to avoid
inconsistencies.

Finally, once the execution environment is correctly configured, additional
clarity is needed on how to *run* the project itself. Which packages should be
loaded first? Which scripts should be executed, and in what order? Without clear
documentation or automated orchestration, these operational details become yet
another barrier to reproducibility.

This paper focuses on two critical but often overlooked aspects of
reproducibility: *computational environment management* and *workflow
orchestration*. We present a comprehensive framework addressing both challenges
using the [Nix]{.pkg} package manager [@dolstra2004nix], making it accessible to
researchers through two [R]{.proglang} packages: [rix]{.pkg} and
[rixpress]{.pkg}. Before introducing these packages, we survey existing tools and
their limitations to contextualise our contributions.

A range of tools now exist to help researchers approach the gold standard of
full reproducibility, or to consistently deploy the same development environment
across multiple machines. Let us first consider the most basic step in this
process: listing the software used. In [R]{.proglang}, the `sessionInfo()`
function provides a concise summary of the software environment, including the R
version, platform details, and all loaded packages. Its output can be saved to a
file and included as part of a study’s reproducibility record. Below is an
example output from `sessionInfo()`:

::: {.content-hidden when-format="pdf"}
```R
sessionInfo()
```
:::

::: {.content-hidden when-format="pdf"}
```R
R version 4.3.2 (2023-10-31)
Platform: aarch64-unknown-linux-gnu (64-bit)
Running under: Ubuntu 22.04.3 LTS

Matrix products: default
BLAS:   /usr/lib/aarch64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/aarch64-linux-gnu/openblas-pthread/libopenblasp[...]

locale:
 LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 ...

attached base packages:
 stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
 nnet_7.3-19  mgcv_1.9-0   nlme_3.1-163

loaded via a namespace (and not attached):
 compiler_4.3.2  Matrix_1.6-1.1  tools_4.3.2     splines_4.3.2
 grid_4.3.2      lattice_0.21-9
```
:::


```{=tex}
\begin{CodeInput}
R> sessionInfo()
\end{CodeInput}
\begin{CodeOutput}
R version 4.3.2 (2023-10-31)
Platform: aarch64-unknown-linux-gnu (64-bit)
Running under: Ubuntu 22.04.3 LTS

Matrix products: default
BLAS:   /usr/lib/aarch64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/aarch64-linux-gnu/openblas-pthread/libopenblasp[...]

locale:
 LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 ...

attached base packages:
 stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
 nnet_7.3-19  mgcv_1.9-0   nlme_3.1-163

loaded via a namespace (and not attached):
 compiler_4.3.2  Matrix_1.6-1.1  tools_4.3.2     splines_4.3.2
 grid_4.3.2      lattice_0.21-9
\end{CodeOutput}
```

When an author includes this information, others attempting to reproduce the
study (future readers, collaborators, or even the author at a later time) can
see which version of [R]{.proglang} and which packages (with their versions)
were used. However, reproducing the environment still requires manually
installing the correct package versions --- a process that becomes difficult when
packages depend on system-level libraries. For example, the [sf]{.pkg} package
requires [GDAL]{.pkg}, [GEOS]{.pkg}, and [PROJ]{.pkg} libraries. Installing and
configuring these dependencies varies substantially across operating systems and
can be prohibitively difficult on some platforms.

A more robust approach than simply listing package versions is to use tools that
define per-project environments. This is common for [Python]{.proglang} [@python3] users,
who have many tools at their disposal, such as the built-in *virtual
environments* module, or third-party tools like [uv]{.pkg} [@uv], which manages both
project-specific interpreters and package libraries using a lock file mechanism.

In the [R]{.proglang} ecosystem, virtual environments are not built-in as they
are in [Python]{.proglang}, however tools like [renv]{.pkg} provide equivalent
functionality. [renv]{.pkg} captures the project’s software state and writes it
to a lock file (`renv.lock`), which includes the exact versions of
[R]{.proglang} and all required packages. This lockfile serves as a blueprint
for restoring the environment automatically, ensuring others can recreate the
same setup with minimal effort.

However, both [renv]{.pkg} and [uv]{.pkg} have limitations. While [uv]{.pkg}
manages [Python]{.proglang} interpreter versions directly, [renv]{.pkg} does not
restore the version of [R]{.proglang} itself—installing the correct version must
be done separately using tools like [rig]{.pkg} [@rlib2023]. More critically,
neither handles system-level dependencies. If [sf]{.pkg} requires [GDAL]{.pkg}
version 3.0 but the system has version 2.4 installed, neither tool can resolve
this conflict. Users must manually install system libraries, and the process
differs across operating systems.

Other [R]{.proglang} packages such as
[groundhog]{.pkg} [@simonsohn2023] and [rang]{.pkg} [@chan2023] enable
date-based package installation, while the Posit Package Manager provides dated
CRAN snapshots. However, like [renv]{.pkg}, these focus on R packages and do not
address system-level dependencies or R version management. 
Furthermore, using pre-compiled binaries of packages simplifies installation but do not eliminate the need for
system libraries required at runtime. These tools represent significant progress in reproducibility within their
ecosystems, but they share a common limitation: none handles system-level
dependencies or the increasingly polyglot nature of modern data science. 

Indeed, there is evidence that the era of a single tool dominating an entire analytical pipeline is giving 
way to a more pragmatic approach where researchers combine the complementary 
strengths of multiple languages [@peng2018]. This trend is particularly evident
in the synergistic use of [R]{.proglang}, with its rich ecosystem for statistical modeling, 
[Python]{.proglang}, with its extensive libraries for machine learning, and [Julia]{.proglang} 
for complex simulations through the use of interoperability libraries such as [PythonCall]{.pkg} 
(calling [Python]{.proglang} from [Julia]{.proglang}), [JuliaCall]{.pkg} 
(calling [Julia]{.proglang} from [Python]{.proglang} or [R]{.proglang}), [reticulate]{.pkg} 
(calling [Python]{.proglang} from [R]{.proglang}), [rpy2]{.pkg} and [ryp]{.pkg} (calling [R]{.proglang} from [Python]{.proglang}) [@osborne2024, @navarro2024].
Recent survey data confirms this trend, showing that research scientists use, on average, 
nearly two programming languages in their work, with the [R]{.proglang} and [Python]{.proglang} combination being 
the most prevalent [@chen2025]. This creates a significant challenge for reproducibility:
a project using [R]{.proglang}, [Python]{.proglang}, and system tools like
[Quarto]{.pkg} or \LaTeX requires coordinating multiple, disparate package managers, 
each with its own configuration and potential for conflict and using interoperability
libraries.

The demand for solutions that can manage this complexity is not merely academic; 
it is reflected in major strategic shifts within the industry. A prominent example is the 
evolution of Posit (formerly RStudio), which has pivoted from its origins as a premier R-centric 
service and software supplier to a company that treats [Python]{.proglang} as a 
first-class citizen alongside [R]{.proglang}. This industry trend, coupled with the proliferation of academic research
into polyglot tooling, underscores a critical reality: the central challenge for modern reproducible
science is no longer *if* polyglot workflows should be used, but *how* they can be managed in a unified, 
efficient, and reproducible manner.

The most comprehensive approach to managing a complete computational environment
to date is containerization, for which [Docker]{.pkg} has become the de facto
standard. Docker's principal strength lies in its capacity to package an entire
data product—including the operating system, all system-level libraries,
programming language runtimes, and packages—into a single, self-contained
artifact known as an image. Once this image is built, the analysis can be
executed within a container, which is a running instance of that image. This
methodology effectively solves the problem of missing system-level dependencies;
because Docker images are typically minimal Linux systems, they can bundle the
specific versions of libraries like [GDAL]{.pkg} or [GEOS]{.pkg} required by
packages such as [sf]{.pkg}. These images can then be shared via public
registries, enabling a high degree of reproducibility across different host
systems. The Rocker project, for instance, provides invaluable pre-built Docker
images tailored for the [R]{.proglang} community, offering a convenient
foundation for reproducible research environments [@boettiger2017].

However, while Docker excels at providing this form of spatial reproducibility,
its application in interactive research workflows reveals several significant
limitations. First, the container model is often poorly aligned with the
iterative, interactive nature of data analysis. Running graphical applications
like RStudio Desktop requires complex and platform-specific configuration, while
ephemeral container filesystems mean that changes made during a session are lost
by default. Consequently, a common workflow involves developing an analysis
interactively on a host system and only containerizing it post-hoc, which
achieves reproducibility as an afterthought rather than as a continuous
guarantee during the research process. Packages like @dockerfiler are specifically
made to enable this type of post-hoc reproducibility.

Second, the effective use of [Docker]{.pkg} introduces a considerable learning
curve. Authoring a reliable and truly reproducible `Dockerfile` requires a
degree of familiarity with Linux system administration. Best practices, such as
referencing base images by their immutable content digests rather than mutable
tags, are necessary for long-term stability but are not widely adopted.
Furthermore, while [Docker]{.pkg} can house polyglot environments, it offers no native
mechanism for orchestrating a pipeline where distinct steps might require
different, and potentially conflicting, sets of dependencies. The entire
analysis is typically confined to a single, monolithic environment.

The most critical limitation, however, lies in its guarantees of temporal
reproducibility. As noted by @dellaieraMasterThesis2024, reproducibility is best
viewed as a spectrum. Docker provides strong run-time reproducibility
(consistency across space), but offers weaker build-time reproducibility
(consistency over time). The imperative commands within a `Dockerfile`, such as
`apt-get update`, are non-deterministic by nature and can introduce temporal
drift, where rebuilding the same file at different times yields different binary
artifacts. As empirically demonstrated by @malka2025, achieving deterministic
builds requires a system designed for this purpose, a role for which functional
package managers like [Nix]{.pkg} are architecturally better suited.^[This
contrast between the procedural, imperative commands of a `Dockerfile` and the
declarative, functional model of [Nix]{.pkg} highlights a fundamental dichotomy
in approaches to reproducibility, a concept we will examine in detail as we
introduce the [Nix]{.pkg} package manager in Section @sec-nix.]

This is not to say that [Docker]{.pkg} has no place in a reproducible research
workflow. Its utility for production deployment, continuous integration, and
cross-platform distribution is well-established. Indeed, the two approaches are
not mutually exclusive. As shall be explained in @sec-nix, it is possible
to use [Nix]{.pkg} within [Docker]{.pkg} to benefit from the advantages of
both tools.

Making sure the development environment is reproducible is but one piece of the
puzzle, though. Even with a perfectly reproducible environment, researchers face
another challenge: reliably executing the analysis workflow. Which scripts run
first? What are the dependencies between steps? Manual execution is error-prone
and poorly documented. Build automation tools like [Make]{.pkg} address this by
defining analyses as ordered, reproducible steps.

Within [R]{.proglang}, the [targets]{.pkg} package [@landau2021] provides a
modern, declarative approach to workflow management. It tracks dependencies,
caches results, and only recomputes affected steps, improving efficiency for
complex analyses. However, [targets]{.pkg} operates within a single
[R]{.proglang} session. For truly polyglot pipelines—where steps may require
incompatible dependencies or different language versions—this becomes
problematic. Running [targets]{.pkg} inside a [Docker]{.pkg} container ensures
reproducibility but forces the entire pipeline to share one environment.

The current state-of-the-art in reproducible research exemplifies the complexity
of navigating this fragmented landscape. The work of @mcdermott2021 provides an
excellent case study in best practices, with an accompanying repository that is
fully reproducible.^[See
[https://github.com/grantmcdermott/skeptic-priors](https://github.com/grantmcdermott/skeptic-priors)]
Achieving this gold standard, however, required the
masterful integration of multiple complex and disparate tools: [renv]{.pkg} for
managing [R]{.proglang} packages, [Docker]{.pkg} for containerizing the system
environment, and [Make]{.pkg} for orchestrating the analysis. This demonstrates
that while a high degree of reproducibility is attainable, it requires a heroic
effort, forcing researchers to become experts in a brittle and complex
toolchain.

Furthermore, the complexity increases for polyglot projects, which must manage
multiple ecosystems simultaneously.

Researchers therefore need a tool that:

1. Manages complete environments including languages, packages, and system
   dependencies;
2. Supports multiple languages natively;
3. Works interactively without container complexity;
4. Provides step-level isolation for different environments;
5. Ensures bit-for-bit reproducibility through deterministic builds;
6. Remains simple enough for researchers without systems administration
   expertise.

The [Nix]{.pkg} package manager provides these capabilities. It ensures
reproducible installation by deploying *component closures*—packages bundled
with all their direct and transitive dependencies [@dolstra2004nix]. Think of
installing a package as packing for a trip: traditional package managers assume
you’ll find essentials at your destination, while [Nix]{.pkg} packs everything
you need. This produces self-contained software environments that behave
identically across machines and over time.

[Nix]{.pkg} can replace [Docker]{.pkg} for isolation, [renv]{.pkg} for package
management, and even [Make]{.pkg} for orchestration—all within a single, unified
framework. However, it has a steep learning curve, with its own functional
language for declaratively defining software builds and configurations. While
this ensures reproducibility, it also creates a barrier to adoption.

To make [Nix]{.pkg} accessible to researchers, we developed two [R]{.proglang}
packages:

- [rix]{.pkg} generates [Nix]{.pkg} expressions from intuitive R function calls,
  eliminating the need to learn the [Nix]{.proglang} language. It handles
  environment definitions including [R]{.proglang}, [Python]{.proglang},
  [Julia]{.proglang} [@bezanson2017julia], system tools, and dependencies.

- [rixpress]{.pkg} orchestrates polyglot analytical pipelines using [Nix]{.pkg}
  as the build engine. Each step runs in its own hermetic environment with
  automatic caching and dependency tracking, enabling true polyglot workflows
  across [R]{.proglang}, [Python]{.proglang}, and [Julia]{.proglang}.
  [Python]{.proglang} users can use [ryxpress]{.pkg}, a direct port.

Together, these packages provide a single framework for managing reproducible,
polyglot environments and executing complex workflows with step-level isolation.
While [Nix]{.pkg} remains complex for advanced use, [rix]{.pkg} and
[rixpress]{.pkg} abstract this complexity, making deep reproducibility
accessible without systems administration expertise.

The remainder of this paper proceeds as follows. Section 2 introduces
[Nix]{.pkg} and explains how its functional model enables reproducibility.
Section 3 presents [rix]{.pkg} and demonstrates environment definition. Section
4 discusses the `rstats-on-nix` fork of the package repository. Section 5
introduces [rixpress]{.pkg} and demonstrates polyglot pipeline orchestration.
Section 6 concludes with discussion of future directions.

## The Nix Package Manager {#sec-nix}

[Nix]{.pkg} is a cross-platform package manager designed for reproducible
software installation and deployment. Unlike traditional package managers,
[Nix]{.pkg} emphasizes immutable infrastructure and a functional programming
paradigm to ensure computational environment consistency. The primary
[Nix]{.pkg} package collection, `nixpkgs`, provides access to over 120,000
packages, including nearly all of CRAN and Bioconductor.^[Only about 80 packages 
currently fail to build on the NixOS build farm, plus roughly 30 packages marked as broken
by the maintainers of the [R]{.proglang} ecosystem. These could, in principle, be fixed, 
but doing so would require significant effort. Marking them as broken signals to users 
that their build failures are known and intentional. It should be further noted that some
packages may build successfully, but then fail at runtime as they try to further install
runtime dependencies on first run. One such example is [torch]{.pkg}.] According to Repology,
a service tracking package repositories across distributions, `nixpkgs` is both
the largest and among the most up-to-date repositories available (Figure
@fig-repology).

![Package repository size and freshness according to Repology. `nixpkgs` leads
in both total package count and update recency, making it well-suited for
reproducible research environments.](images/repology.png){#fig-repology}

This extensive coverage allows users to install not only [R]{.proglang}
but also all required packages and system-level dependencies for any given
project.

A key advantage of [Nix]{.pkg} over traditional installation methods lies in its
comprehensive dependency management. Consider the [sf]{.pkg} package for spatial
data analysis in [R]{.proglang}. It requires several complex system libraries:
[GDAL]{.pkg}, [GEOS]{.pkg}, and [PROJ]{.pkg}. Manually installing and
configuring these libraries is challenging and platform-specific. With
[Nix]{.pkg}, users simply declare [sf]{.pkg} as a project requirement, and
[Nix]{.pkg} automatically installs and configures all necessary system libraries
as transitive dependencies.

This seamless process stems from [Nix]{.pkg}'s concept of *component closures*.
As @dolstra2004nix explains:

> The idea is to always deploy component closures: if we deploy a component,
> then we must also deploy its dependencies, their dependencies, and so on.
> [...] Since closures are self-contained, they are the units of complete
> software deployment.

In other words, every package deployment includes everything it needs to run,
with no hidden dependencies on the host system.

While comprehensive dependency tracking could theoretically be implemented in
any package manager, what distinguishes [Nix]{.pkg} (alongside its cousin
[Guix]{.pkg}) is its *functional package management* paradigm. Traditional
package managers require imperative, step-by-step installation instructions: *do
this, then this, then that*. [Nix]{.pkg} inverts this model; users declare the
desired end state (*I require an environment with these packages*), and
[Nix]{.pkg} determines how to achieve it.

This is possible because [Nix]{.pkg} packages are defined as *derivations*:
declarative blueprints specifying all inputs (source code, build commands,
dependencies) needed to build a package. When installing software, [Nix]{.pkg}
evaluates expressions written in the [Nix]{.proglang} language to produce these
derivations. The declarative nature ensures every build process is fully
specified and independent of the execution environment.

For example, the [rJava]{.pkg} package definition includes the correct
`JAVA_HOME` configuration pointing to the [jdk]{.pkg} dependency. Outside
[Nix]{.pkg}, users must manually set this variable if automatic detection fails.
With [Nix]{.pkg}, the package maintainers handle this complexity once (by
writing the correct [Nix]{.pkg} expressions that perform the installation
purely), and all users benefit.

[Nix]{.pkg} conceptualizes software builds as *pure functions* from functional
programming. In mathematics, a pure function always produces the same output for
a given input: if $f(x) = x^2$, then $f(2) = 4$ invariably holds. Analogously,
[Nix]{.pkg} ensures that identical inputs (source code, dependencies, build
instructions) produce identical outputs (built packages), regardless of when or
where the build occurs.

This determinism is achieved through hermetic (isolated) builds with no hidden
dependencies on global system state nor undeclared side effects. The
effectiveness of this model was validated at unprecedented scale by @malka2025,
who rebuilt 709,816 packages from historical `nixpkgs` snapshots (2017–2023),
finding rebuildability rates exceeding 99% and bitwise reproducibility rates
between 69% and 91%, with an upward trend. Importantly, about 15% of
unreproducible builds failed due to embedded timestamps—a solvable
implementation issue rather than a fundamental model limitation.

All [Nix]{.pkg} package definitions are hosted in the `nixpkgs` GitHub
repository. By *pinning* to a specific commit (or equivalently, a specific
date), users guarantee that all installed packages derive from identical build
instructions. An environment built today using a pinned revision will produce
the same software versions and configurations if rebuilt years later, provided
the `nixpkgs` repository remains accessible (it is archived and distributed
across multiple mirrors).

[Nix]{.pkg} installs all packages into a content-addressed directory called the
Nix store (typically `/nix/store`). Each package occupies a unique subdirectory
whose name includes a cryptographic hash of all its inputs: source code,
dependencies, and build instructions. This means `/nix/store/abc123-dplyr-1.0.0`
and `/nix/store/xyz789-dplyr-1.1.0` can coexist without conflicts, allowing
multiple [R]{.proglang} versions on the same system and eliminating "dependency
hell."

While this functional approach enhances reproducibility, it can introduce
complexity for package maintainers, particularly for software downloading
external assets during installation (e.g., certain Bioconductor packages). For
end-users, however, this complexity is largely abstracted away by [rix]{.pkg}
and [rixpress]{.pkg}, which we introduce in the following sections.

For a more in-depth technical discussion of [Nix]{.pkg}'s design principles, see
@dolstra2004nix.

By leveraging these principles, [Nix]{.pkg} provides a unified framework that
can subsume the functionalities of disparate tools. For [R]{.proglang} projects,
it can replace the combination of [renv]{.pkg} for package management and
[Docker]{.pkg} for system-level isolation. Similarly, for [Python]{.proglang}
projects, it offers a more comprehensive alternative to `requirements.txt` files
and virtual environments. Furthermore, [Nix]{.pkg} excels at composing polyglot
environments, seamlessly integrating [R]{.proglang}, [Python]{.proglang},
[Julia]{.proglang}, a [LaTeX]{.proglang} distribution, and any of the numerous
other tools available in `nixpkgs`. This enables the creation of a truly
complete, project-specific, and deeply reproducible environment suitable for
both interactive development and non-interactive, automated analysis. The
long-term viability of this reproducibility is contingent only on the continued
accessibility of the `nixpkgs` repository or a suitable fork, as will be
discussed in Section @sec-fork.

[Nix]{.pkg} offers robust cross-platform support, running natively on Linux
(x86_64) with optimal functionality. On macOS (Intel and Apple Silicon),
however, its reproducibility guarantees are somewhat compromised by dependencies
on proprietary system frameworks and Xcode toolchains that reside outside
of the [Nix]{.pkg} store; consequently, macOS users may need to update project
pins following system upgrades. For Windows users, [Nix]{.pkg} operates
effectively through the Windows Subsystem for Linux (WSL2), where environments
can be integrated into development workflows using editors like VS Code or
Positron.

While [Nix]{.pkg} is a powerful system, its adoption is not without notable
challenges. The most significant is its steep learning curve; it uses its own
programming language, also called [Nix]{.proglang}, which is purely functional, a
paradigm that may be unfamiliar to many researchers. This complexity is most
apparent when authoring custom build instructions or troubleshooting intricate
dependency issues. Additionally, build times can be substantial for large
environments when pre-built binary artifacts are not available from a cache. The
immutable, content-addressed nature of the [Nix]{.pkg} store also leads to
higher disk usage compared to traditional package managers. Finally, while the
`nixpkgs` repository is extensive, certain niche packages may be unavailable or
fail to build on all platforms.

Beyond these technical aspects, a practical hurdle to adoption is the ubiquity
of [Docker]{.pkg}. Given the extensive infrastructure and community knowledge
built around containerization, [Nix]{.pkg} can be perceived as a competing
rather than a complementary technology. This perception, however, is a false
dichotomy. [Docker]{.pkg} and [Nix]{.pkg} allow researchers to solve similar
issues but these tools are not alternatives of one another. Thus, a powerful
hybrid model involves using [Nix]{.pkg} to deterministically build the
environment *during the construction of a [Docker]{.pkg} image*, which is then
deployed using standard container infrastructure. This approach leverages the
strengths of both tools: [Nix]{.pkg} provides robust build-time reproducibility,
while [Docker]{.pkg} serves as a universal distribution and deployment
mechanism. For many research workflows, however, where interactive development
is paramount and the overhead of containerization is a barrier, the unified
model provided by [Nix]{.pkg} alone can provide a more direct and streamlined
path to reproducibility.

To make these capabilities more approachable for [R]{.proglang} users seeking
reproducible, project-specific environments without learning the full
[Nix]{.proglang} language, we created the [rix]{.pkg} and [rixpress]{.pkg}
packages.

## Reproducible development environments with Nix {#sec-repro-nix}

As mentioned, [Nix]{.pkg} expressions are written in the [Nix]{.proglang}
programming language, which is purely functional. Here is a simple example that
creates a shell environment containing version 4.3.1 of [R]{.proglang}:

::: {.content-hidden when-format="pdf"}
```nix
let
  pkgs = import (fetchTarball
    "https://github.com/NixOS/nixpkgs/archive/976fa336.tar.gz"
  ) {};
  system_packages = builtins.attrValues {
    inherit (pkgs) R;
  };
in
  pkgs.mkShell {
    buildInputs = [ system_packages ];
    shellHook = "R --vanilla";
  }
```
:::


```{=tex}
\begin{CodeInput}
let
  pkgs = import (fetchTarball
    "https://github.com/NixOS/nixpkgs/archive/976fa336.tar.gz"
  ) {};
  system_packages = builtins.attrValues {
    inherit (pkgs) R;
  };
in
  pkgs.mkShell {
    buildInputs = [ system_packages ];
    shellHook = "R --vanilla";
  }
\end{CodeInput}
```

In this expression, the `let` keyword is used to define variables. The variable
`pkgs` imports the set of packages from the `nixpkgs` repository at the
specified commit `976fa336`. The variable `system_packages` lists the packages
to include in the environment; in this case, it is just the [R]{.proglang}
programming language, along with all its dependencies and their transitive
dependencies. The `mkShell` function then creates a development shell with the
specified packages. The `shellHook` is set to `"R --vanilla"`, meaning that
entering the shell automatically starts [R]{.proglang} in vanilla mode, ignoring
any startup options.

This expression can be saved in a file called `default.nix`. The environment can
then be built on a system with [Nix]{.pkg} installed using the `nix-build`
command.^[For installing [Nix]{.pkg}, we recommend the Determinate Systems
installer: \url{https://determinate.systems/posts/determinate-nix-installer}].
Once the build completes, the user can enter the interactive shell with
`nix-shell`, which executes the following steps:

- [Nix]{.pkg} reads the expression and identifies what needs to be installed;
- It checks if these packages already exist in `/nix/store`;
- If not, it downloads or builds them (with all dependencies);
- It creates a new shell environment and updates the `$PATH` to include the
  right binaries from `/nix/store`.
- The user is now in an interactive [Nix]{.pkg} shell session.

This shell contains all the packages specified in `default.nix` and
can be used for development, similar to activating a virtual environment in the
[Python]{.proglang} ecosystem.

Writing [Nix]{.pkg} expressions can be challenging for users unfamiliar with the
[Nix]{.proglang} language. However, the ability to define a fully reproducible
development environment in a single text file and then rebuild it anywhere is
highly appealing. [rix]{.pkg} aims to lower the barrier to adoption of
[Nix]{.pkg} for reproducibility.

[rix]{.pkg}, an [R]{.proglang} package, provides the `rix()` function, which
simplifies generating [Nix]{.pkg} expressions. It is available on CRAN and can
be installed like any other [R]{.proglang} package. Additionally, it can
bootstrap an [R]{.proglang} development environment on a system where
[R]{.proglang} is not yet installed but [Nix]{.pkg} is available. This can be
done by running (inside of a terminal):

::: {.content-hidden when-format="pdf"}
```bash
nix-shell -I \
  nixpkgs=https://github.com/rstats-on-nix/nixpkgs/tarball/2025-10-20 -p \
  R rPackages.rix
```
:::


```{=tex}
\begin{CodeInput}
$> nix-shell -I \
+    nixpkgs=https://github.com/rstats-on-nix/nixpkgs/tarball/2025-10-20 -p \
+    R rPackages.rix
\end{CodeInput}
```

(the `-I` flag allows one to pass a specific revision of `nixpkgs`, ensuring
temporary shells are also reproducible).

This command opens a temporary [R]{.proglang} session with [rix]{.pkg}
available.^[`nix-shell -p` starts an interactive shell with the specified
packages.] From there, users can generate new [Nix]{.pkg} expressions for
building environments. For example, the following generates a `default.nix` file
that installs [R]{.proglang} 4.3.1 along with the [dplyr]{.pkg} and
[chronicler]{.pkg} packages:

::: {.content-hidden when-format="pdf"}
```r
library('rix')

rix(r_ver = "4.3.1",
  r_pkgs = c("dplyr", "chronicler"),
  project_path = ".",
  overwrite = TRUE)
```
:::

```{=tex}
\begin{CodeInput}
R> library('rix')

R> rix(r_ver = "4.3.1",
+    r_pkgs = c("dplyr", "chronicler"),
+    project_path = ".",
+    overwrite = TRUE)
\end{CodeInput}
```

[rix]{.pkg} can also handle more complex setups, and users can provide a date
instead of a specific [R]{.proglang} version:

::: {.content-hidden when-format="pdf"}
```r
rix(date = "2025-10-20",
  r_pkgs = c("rix", "dplyr", "chronicler", "AER@1.2-8"),
  system_pkgs = c("quarto", "git"),
  tex_pkgs = c(
        "amsmath",
        "framed",
        "fvextra",
        "environ",
        "fontawesome5",
        "orcidlink",
        "pdfcol",
        "tcolorbox",
        "tikzfill"
  ),
  git_pkgs = list(
      package_name = "fusen",
      repo_url = "https://github.com/ThinkR-open/fusen",
      commit = "60346860111be79fc2beb33c53e195f97504a667"
  ),
  ide = "positron",
  project_path = ".",
  overwrite = TRUE)
```
:::

```{=tex}
\begin{CodeInput}
R> rix(date = "2025-10-20",
+    r_pkgs = c("rix", "dplyr", "chronicler", "AER@1.2-8"),
+    system_pkgs = c("quarto", "git"),
+    tex_pkgs = c(
+          "amsmath",
+          "framed",
+          "fvextra",
+          "environ",
+          "fontawesome5",
+          "orcidlink",
+          "pdfcol",
+          "tcolorbox",
+          "tikzfill"
+    ),
+    git_pkgs = list(
+        package_name = "fusen",
+        repo_url = "https://github.com/ThinkR-open/fusen",
+        commit = "60346860111be79fc2beb33c53e195f97504a667"
+    ),
+    ide = "positron",
+    project_path = ".",
+    overwrite = TRUE)
\end{CodeInput}
```

This call to `rix()` generates a `default.nix` file for a development shell that
encapsulates a complete and reproducible research environment. It provides
[R]{.proglang} and its packages (including [AER]{.pkg} at version 1.2-8),
several TeXLive packages for \LaTeX document authoring, development versions of
[rix]{.pkg} and [fusen]{.pkg} pulled directly from GitHub at a specific commit,
and the Positron editor. The reproducibility of this environment is guaranteed
by pinning all components to a single point in time: the [R]{.proglang} packages
are resolved from the CRAN snapshot of October 20th 2025, while all other system
tools are fixed to the version of `nixpkgs` from that same date. Typing
`nix-shell` in a terminal within the folder that contains this `default.nix`
will drop the user into a new shell. It is also possible to configure IDEs to
dynamically load this new shell to provide the proper development tooling for
better interactive use.

It is also possible to include [Python]{.proglang} and [Julia]{.proglang}
packages using the `py_conf` and `jl_conf` arguments respectively.
[Julia]{.proglang} packages work the same way as [R]{.proglang} packages: they
are pinned to their versions as of the specified date.

[Python]{.proglang} packages, however, require special handling. Unlike CRAN or
the Julia package registry, PyPI (the [Python]{.proglang} Package Index)
contains over 500,000 packages with highly variable quality—many lack proper
build specifications, have broken dependencies, or are abandoned. Because of
this heterogeneity, [Nix]{.pkg} maintainers cannot automatically package all of
PyPI as they do for CRAN or the [Julia]{.proglang} registry. Instead, they
manually curate and package individual [Python]{.proglang} packages that meet
quality standards.^[On October 2025, this amounts to roughly 10000 packages, 
according to the search engine for [Nix]{.pkg} packages.] 
This means some [Python]{.proglang} packages, or specific
versions, may not be available through [Nix]{.pkg}.

For projects requiring [Python]{.proglang} packages not available in
[Nix]{.pkg}, [rix]{.pkg} supports integration with [uv]{.pkg}, a modern
[Python]{.proglang} package manager. This allows users to combine Nix's
system-level reproducibility ([Python]{.proglang} interpreter, system libraries)
with [uv]{.pkg}'s comprehensive package availability. While this hybrid approach
sacrifices some of Nix's deterministic build guarantees for [Python]{.proglang}
packages, it provides a pragmatic solution when the needed packages are
unavailable in `nixpkgs`.

Users can include [uv]{.pkg} as a system package:

::: {.content-hidden when-format="pdf"}
```r
rix(date = "2025-10-20",
  r_pkgs = c("rix", "dplyr", "chronicler"),
  system_pkgs = c("uv"),
  project_path = ".",
  overwrite = TRUE)
```
:::
```{=tex}
\begin{CodeInput}
R> rix(date = "2025-10-20",
+    r_pkgs = c("rix", "dplyr", "chronicler"),
+    system_pkgs = c("uv"),
+    project_path = ".",
+    overwrite = TRUE)
\end{CodeInput}
```

This hybrid approach offers complementary strengths: [Nix]{.pkg} manages the
[Python]{.proglang} interpreter, system libraries (e.g., [GDAL]{.pkg},
[HDF5]{.pkg}), and [R]{.proglang} packages with full determinism, while
[uv]{.pkg} handles [Python]{.proglang} packages through its own lockfile
mechanism (`uv.lock`). Within the Nix environment, users run standard [uv]{.pkg}
commands (e.g., `uv pip install pandas==2.0.0`) to install [Python]{.proglang}
packages. The [uv]{.pkg} lockfile records exact package versions and their
hashes, providing reproducibility for the [Python]{.proglang} package layer.

While this approach doesn't achieve Nix's level of build-time determinism for
[Python]{.proglang} packages ([uv]{.pkg} still downloads wheels or builds
packages at install time rather than using pre-built, content-addressed
artefacts) it provides a pragmatic balance: comprehensive [Python]{.proglang}
package availability with reasonable reproducibility guarantees. For polyglot
projects, this means [R]{.proglang} and system-level dependencies remain fully
reproducible through [Nix]{.pkg}, while [Python]{.proglang} packages gain the
reproducibility that [uv]{.pkg} can provide. We recommend of course to use
[Python]{.pkg} through [Nix]{.pkg} whenever possible and only resort to the
hybrid approach with [uv]{.pkg} as a last resort.^[It is also possible to package
the missing [Python]{.pkg} for [Nix]{.pkg} and submit a pull request to `nixpkgs`,
but we recognise that this option may not be beginner-friendly.]

Anywhere [Nix]{.pkg} can be installed, users can benefit from its features. For
instance, the repository containing the source code for this
article^[https://github.com/b-rodrigues/rix_paper] uses GitHub Actions to
compile the paper. Each time a push is made to the master branch, a runner
installs [Nix]{.pkg}, generates the environment from the hosted `default.nix`
file, and compiles the paper using [Quarto]{.pkg} within the reproducible
environment. This ensures that *exactly* the same environment is used on the
authors' machines and on the CI/CD platform without any additional,
platform-specific, configuration.

Instead of first entering a [Nix]{.pkg} shell, it is also possible to run a
program directly from the environment:

::: {.content-hidden when-format="pdf"}
```bash
cd /path/to/project/ && nix-shell default.nix --run "Rscript analysis.R"
```
:::

```{=tex}
\begin{CodeInput}
cd /path/to/project/ && nix-shell default.nix --run "Rscript analysis.R"
\end{CodeInput}
```

This command runs `Rscript` and executes the `analysis.R` script, which in this
example should be located in the same directory as `default.nix`.

## The rstats-on-nix fork of nixpkgs {#sec-fork}

As explained earlier, [Nix]{.pkg} uses expressions from the `nixpkgs` GitHub
repository to build software. However, when generating expressions with
[rix]{.pkg}, the fork `rstats-on-nix/nixpkgs` is used instead.

Using a fork offers several advantages. First, it provides flexibility that the
official `nixpkgs` repository cannot always accommodate. Because [Nix]{.pkg}
serves as the package manager for the NixOS Linux distribution, upstream
priorities center on system-level stability rather than rapid updates to
language ecosystems like [R]{.proglang} or [Julia]{.proglang}.

For instance, while [Nix]{.pkg} can theoretically support multiple versions (or
*variants*) of the same package, in practice maintainers cannot provide several
variants for all [R]{.proglang} packages, given the size of the ecosystem (over
20,000 CRAN and Bioconductor packages). This makes it difficult to install a
specific version of an [R]{.proglang} package not included in a particular
`nixpkgs` commit. With [rix]{.pkg}, users can install a specific package version
from source, e.g.:

::: {.content-hidden when-format="pdf"}
```r
rix(..., r_pkgs = "dplyr@1.0.7", ...)
```
:::

```{=tex}
\begin{CodeInput}
R> rix(..., r_pkgs = "dplyr@1.0.7", ...)
\end{CodeInput}
```

However, installing from source might fail, especially if the package needs to
be compiled.

Additionally, updating the full [R]{.proglang} package set on [Nix]{.pkg} daily
is impractical. While CRAN and Bioconductor update daily, the [R]{.proglang}
packages in `nixpkgs` are only updated with new [R]{.proglang} releases. This
limitation is due to [Nix]{.pkg}’s governance as a Linux distribution package
manager.

The `rstats-on-nix` fork allows us to circumvent these limitations. For example,
it provides daily snapshots of CRAN and the [Julia]{.proglang}. Each day, the
[R]{.proglang} package set is updated and committed to a dated branch using
GitHub Actions. Users can select a specific date with:

::: {.content-hidden when-format="pdf"}
```r
rix(date = "2024-12-14", ...)
```
:::

```{=tex}
\begin{CodeInput}
R> rix(date = "2024-12-14", ...)
\end{CodeInput}
```

We strive to provide an available date per week: each Monday, a GitHub Action
tests popular [R]{.proglang} packages on Linux and macOS, and only if all tests
succeed is the date added to the list of available dates in [rix]{.pkg}. Users
can see all available dates with `rix::available_dates()`. This ensures users
can reliably install packages, and allows us to backport fixes if needed. For
example, when RStudio was temporarily broken due to a dependency issue
(`boost`), a pull request was submitted to the official `nixpkgs` repository. I
backported the fix to the `rstats-on-nix` fork, making RStudio available to
users of [rix]{.pkg} earlier than upstream, as merging PRs in the official
repository can take some time.

We have backported fixes to the `rstats-on-nix` `nixpkgs` fork as far back as
March 2019. The process involves checking out a `nixpkgs` commit on the selected
date, updating the [R]{.proglang} package set using Posit CRAN and Bioconductor
snapshots, backporting fixes, and ensuring popular packages work on both
x86-linux (including WSL2) and aarch64-darwin (Apple Silicon). These changes are
committed to a dated branch in `rstats-on-nix/nixpkgs`.

A drawback of forking `nixpkgs` is that backported packages are not included
upstream and thus are not prebuilt by Nix’s CI platform, Hydra. Users may need
to build many packages from source, which can be time-consuming. To mitigate
this, we provide a binary cache sponsored by 
[Cachix](https://www.cachix.org/)^[[https://www.cachix.org/](https://www.cachix.org/)],
complementing the public Nix cache. Instructions for using Cachix are in
[rix]{.pkg}’s documentation. Using the cache significantly speeds up
installations, as prebuilt packages are downloaded rather than compiled.

The fork also allows us to catch issues (such as packages’ builds breaking) early
on, and prepare fixes that can then be contributed upstream.

While the reliance on the `rstats-on-nix` fork might initially raise
sustainability concerns, this risk should be contextualized. The fork serves a
focused purpose: it updates CRAN and [Julia]{.proglang} package registries daily
and backports fixes when builds break. Importantly, users are never locked into
this fork—they retain the ability to point directly to any commit in the
upstream `nixpkgs` repository, maintaining full access to the official
[Nix]{.pkg} ecosystem. The fork introduces no fundamental architectural changes
or proprietary extensions; it functions as a convenience layer providing tested
package sets and dated snapshots. Should maintenance cease, users could
seamlessly transition to upstream `nixpkgs` commits, though with trade-offs:
less frequent [R]{.proglang} and [Julia]{.proglang} package updates. The fork's
value lies not in creating dependency, but in reducing friction through testing
infrastructure and curated "known-good" dates. Thus, the fork represents an
enhancement to user experience rather than a critical single point of
failure—the underlying [Nix]{.pkg} infrastructure and official `nixpkgs`
repository remain the foundation, ensuring long-term viability of environments
created with [rix]{.pkg}.

## Orchestrating the workflow with rixpress {#sec-rixpress}

Defining a reproducible environment with [rix]{.pkg} addresses the first major
challenge of reproducibility: environment capture. The second challenge, reliably
and efficiently executing the analysis workflow within that environment, is
addressed by [rixpress]{.pkg} (or [ryxpress]{.pkg} for [Python]{.proglang}
users).

As mentioned in the introduction, a build automation tool like [targets]{.pkg}
is invaluable for managing complex analyses. It tracks dependencies between code
and data, caches results, and only recomputes steps that have changed. One can
run a [targets]{.pkg} pipeline inside a Nix environment to make it reproducible.
However, this approach has limitations: the entire pipeline must run in a single
environment, and orchestrating steps across different languages (e.g.,
[R]{.proglang} and [Python]{.proglang}) requires manual handling via packages
like [reticulate]{.pkg}.

[rixpress]{.pkg} overcomes these limitations by using [Nix]{.pkg} not just as a
package manager, but as the build automation engine itself. In a
[rixpress]{.pkg} pipeline, each step is defined as a [Nix]{.pkg} derivation,
providing two key benefits:

  1. True Polyglot Pipelines: Each step can have its own [Nix]{.pkg} environment. A
     [Python]{.proglang} step can run in a pure [Python]{.proglang} environment,
     an [R]{.proglang} step in an [R]{.proglang} environment, and a Quarto
     rendering step in yet another, all within the same pipeline.
  2. Deep Reproducibility: Each step is a hermetically sealed [Nix]{.pkg}
     derivation whose output is cached in the [Nix]{.pkg} store based on the
     hash of all its inputs. All artefacts depend directly on their
     computational environment, ensuring that any change in dependencies
     triggers a rebuild.

As an example, we demonstrate a polyglot pipeline that simulates a Real Business
Cycle (RBC) model in [Julia]{.proglang}, trains an [XGBoost]{.pkg} forecasting
model in [Python]{.proglang} on the simulated data, visualises results in
[R]{.proglang}, and compiles a [Quarto]{.pkg} report. The code for this example
can be found in the [following
repository](https://github.com/b-rodrigues/rixpress_demos/tree/master/case-study)^[https://github.com/b-rodrigues/rixpress_demos/tree/master/case-study],
with additional details provided in the Appendix of this manuscript. Although
this example is purely synthetic and not of scientific significance, it
effectively demonstrates the capabilities of [rixpress]{.pkg}.

For new projects, [rixpress]{.pkg} provides `rxp_init()`, which generates the
necessary boilerplate structure including a pipeline definition file and
template helper scripts. Users then define their pipeline in the script
`gen-pipeline.R` using functions inspired by [targets]{.pkg}, and the
computational environment the pipeline will be executed in the `gen-env.R`
script. If functions are needed for pipeline steps, these are to be specified,
in language-specific helper scripts: `functions.jl` for [Julia]{.proglang},
`functions.py` for [Python]{.proglang}, and `functions.R` for [R]{.proglang}.
This separation keeps the pipeline definition clean and readable while
encapsulating the scientific logic in appropriate language-native files.
Importantly, these helper scripts are standard language files—they contain no
[rixpress]{.pkg}-specific code and can be reused in other projects or executed
independently outside of [Nix]{.pkg}. This design allows users to preserve
standard development practices while still benefiting from declarative
orchestration.

The following example demonstrates how [rixpress]{.pkg} orchestrates a granular,
multi-step data science workflow that crosses language boundaries. (Complete
setup instructions and a detailed walkthrough are provided in Appendix)

::: {.content-hidden when-format="pdf"}
```r
library('rixpress')

pipeline_steps <- list(
  # STEP 0: Define RBC Model Parameters in Julia
  rxp_jl(alpha, 0.3),
  rxp_jl(beta, 1 / 1.01),
  # ... (other parameters omitted for brevity) ...

  # STEP 1: Julia - Simulate the RBC model
  rxp_jl(
    name = simulated_rbc_data,
    expr = "simulate_rbc_model(alpha, beta, delta, rho, sigma, sigma_z)",
    user_functions = "functions/functions.jl",
    encoder = "arrow_write"
  ),

  # STEP 2.1: Python - Prepare features
  rxp_py(
    name = processed_data,
    expr = "prepare_features(simulated_rbc_data)",
    user_functions = "functions/functions.py",
    decoder = "pyarrow.feather.read_feather"
  ),

  # STEP 2.2: Python - Split data (X_train, y_train, etc.)
  rxp_py(name = X_train, expr = "get_X_train(processed_data)", ...),
  # ... (other data splits omitted for brevity) ...

  # STEP 2.3: Python - Train the XGBoost model
  rxp_py(
    name = trained_model,
    expr = "train_model(X_train, y_train)",
    user_functions = "functions/functions.py"
  ),

  # STEP 2.4: Python - Make predictions and format results
  # ... (prediction and formatting steps omitted for brevity) ...
  rxp_py(
    name = final_predictions_df,
    expr = "format_results(y_test, model_predictions)",
    user_functions = "functions/functions.py",
    encoder = "save_arrow"
  ),

  # STEP 3: R - Visualise the predictions
  rxp_r(
    name = output_plot,
    expr = plot_predictions(final_predictions_df),
    user_functions = "functions/functions.R",
    decoder = arrow::read_feather
  ),

  # STEP 4: Quarto - Compile the final report
  rxp_qmd(
    name = final_report,
    qmd_file = "readme.qmd"
  )
)

# Generate the 'pipeline.nix' file from the R list
rxp_populate(pipeline_steps, build = TRUE)
```
:::

```{=tex}
\begin{CodeInput}
R> library('rixpress')

R> pipeline_steps <- list(
+  # STEP 0: Define RBC Model Parameters in Julia
+  rxp_jl(alpha, 0.3),
+  rxp_jl(beta, 1 / 1.01),
+  # ... (other parameters omitted for brevity) ...
+
+  # STEP 1: Julia - Simulate the RBC model
+  rxp_jl(
+    name = simulated_rbc_data,
+    expr = "simulate_rbc_model(alpha, beta, delta, rho, sigma, sigma_z)",
+    user_functions = "functions/functions.jl",
+    encoder = "arrow_write"
+  ),
+
+  # STEP 2.1: Python - Prepare features
+  rxp_py(
+    name = processed_data,
+    expr = "prepare_features(simulated_rbc_data)",
+    user_functions = "functions/functions.py",
+    decoder = "pyarrow.feather.read_feather"
+  ),
+
+  # STEP 2.2: Python - Split data (X_train, y_train, etc.)
+  rxp_py(name = X_train, expr = "get_X_train(processed_data)", ...),
+  # ... (other data splits omitted for brevity) ...
+
+  # STEP 2.3: Python - Train the XGBoost model
+  rxp_py(
+    name = trained_model,
+    expr = "train_model(X_train, y_train)",
+    user_functions = "functions/functions.py"
+  ),
+
+  # STEP 2.4: Python - Make predictions and format results
+  # ... (prediction and formatting steps omitted for brevity) ...
+  rxp_py(
+    name = final_predictions_df,
+    expr = "format_results(y_test, model_predictions)",
+    user_functions = "functions/functions.py",
+    encoder = "save_arrow"
+  ),
+
+  # STEP 3: R - Visualise the predictions
+  rxp_r(
+    name = output_plot,
+    expr = plot_predictions(final_predictions_df),
+    user_functions = "functions/functions.R",
+    decoder = arrow::read_feather
+  ),
+
+  # STEP 4: Quarto - Compile the final report
+  rxp_qmd(
+    name = final_report,
+    qmd_file = "readme.qmd"
+  )
+)

# Generate the 'pipeline.nix' file from the R list
R> rxp_populate(pipeline_steps, build = TRUE)
\end{CodeInput}
```

Computation steps, referred to as *derivations*, are defined using the functions
`rxp_r()`, `rxp_jl()`, and `rxp_py()`, corresponding to [R]{.proglang},
[Julia]{.proglang}, and [Python]{.proglang} environments, respectively. Variants
of these functions suffixed with `_file()` designate *import derivations*, which
introduce external data into the pipeline. It is important to note that any data
imported in this way is stored within the [Nix]{.pkg} store, ensuring
reproducibility but also persisting the data in the build environment. The
function `rxp_qmd()` is used to compile a [Quarto]{.pkg} document (there is also
`rxp_rmd()` which serves the same purpose for [R Markdown]{.pkg} documents).

The `rxp_populate()` function translates this [R]{.proglang} list into a
`pipeline.nix` file, which declaratively defines the entire workflow. Data flows
from derivation to derivation by being serialised into the efficient and
language-agnostic Arrow format using the pair of `encoder/decoder` functions.
Other universal formats, such as `csv` or `json` could have been used. It should
also be noted that if interoperability libraries such as [reticulate]{.pkg} or 
[rds2py]{.pkg} are available, it becomes possible to seamlessly transfer
supported objects to and from [R]{.proglang} and [Python]{.proglang} (for now,
such a feature is not yet integrated for conversion of arbitrary [Julia]{.proglang} objects).

As a sidenote: [Python]{.proglang} users who wish to use [ryxpress]{.pkg} define
pipelines using the same [R]{.proglang}-based DSL shown above. This design
choice keeps the pipeline definition language consistent across both
[R]{.proglang} and [Python]{.proglang} ecosystems. [ryxpress]{.pkg} calls
[R]{.proglang} and [rixpress]{.pkg} under the hood to generate and build the
`pipeline.nix` file. To use [ryxpress]{.pkg}, one should simply add this
package to the `py_conf` argument of `rix()`.

![Graphical representation of the polyglot pipeline. Purple nodes are
[Julia]{.proglang}, yellow nodes are [Python]{.pkg}, light blue nodes are
[R]{.proglang}, and dark blue nodes are Quarto
derivations.](images/dag.png){#fig-dag}

The package can then generate a visual representation of the pipeline's directed
acyclic graph by running `rxp_dag()`, as can be seen in @fig-dag.

To execute the pipeline, one can either set `build = TRUE` in `rxp_populate()`
or call `rxp_make()` separately. [Nix]{.pkg} executes each step in order,
building dependencies as needed. Outputs are cached, so subsequent runs only
recompute steps with changed inputs or code. This provides the efficiency of
[targets]{.pkg} with polyglot support and bit-for-bit reproducibility. Artefacts
can be inspected interactively in [R]{.proglang} using
`rxp_read("artefact_name")` or `rxp_load("artefact_name")`.

[rixpress]{.pkg} also includes several additional features not covered here for
brevity. As mentioned previously, it is also possible to configure popular IDEs
to work interactively and seamlessly with both [rix]{.pkg} and [rixpress]{.pkg},
enabling a smooth, reproducible workflow from within the development
environment. Detailed setup instructions are provided in the vignettes of both
packages.

In summary, [rixpress]{.pkg} extends the guarantees of [rix]{.pkg}
from static environments to dynamic workflows, enabling end-to-end, polyglot
reproducibility.

## A Comparison of Pipeline Paradigms: Imperative vs. Declarative {#sec-comparison}

To illustrate the practical benefits of our framework, we implemented the RBC
pipeline example from Section @sec-rixpress using two distinct paradigms. The
first is a traditional, imperative, stack combining Docker for environment
isolation, Make for workflow orchestration, and language-specific package
managers.^[This choice of tools is illustrative: we could have used, for instance,
[Snakemake]{.pkg} [@moelder2021snakemake] for workflow orchestration or [conda]{.pkg} [@anaconda] 
instead of [uv]{.pkg}, 
but the broader point remains—these are all imperative approaches] 
The second is our proposed declarative framework using `rix` and
`rixpress`. While both approaches successfully execute the same polyglot
analysis to produce identical outputs, their comparison illuminates fundamental
differences in conceptual models, system complexity, and the nature of the
reproducibility guarantees they provide.

### The Imperative Composition of Specialized Tools

The traditional approach represents a composition of specialized tools, each
solving part of the reproducibility puzzle. This paradigm is purely
imperative; the researcher must explicitly script the procedural steps for both
environment construction and workflow execution.

The codification of the environment is a procedural build process specified in a
`Dockerfile`. Successfully authoring such a file necessitates that the
researcher act as a system administrator, manually scripting a sequence of shell
commands to install each component. This includes managing `apt` repositories
for R, invoking external installers like `uv` and `rig` for Python and R
runtimes, and using `curl` and `tar` for Julia. Subsequently, separate commands
must be run to install packages within each language's ecosystem. The procedural
verbosity and intertwined concerns of this approach are reflected in a
`Dockerfile` spanning 108 lines to define the complete environment.

Here is a sketch of this `Dockerfile`:

::: {.content-hidden when-format="pdf"}
```r
# Add R repository and install specific version
RUN apt-get update && apt-get install -y software-properties-common
RUN add-apt-repository ppa:...
RUN curl -L https://rig.r-pkg.org/... | sh
RUN rig add 4.5.1

# Install Python with uv
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
RUN uv python install 3.13

# Download and extract Julia
RUN curl -fsSL https://julialang-s3.julialang.org/... -o julia.tar.gz
RUN tar -xzf julia.tar.gz -C /opt/

# Install packages for each language separately
RUN echo 'options(repos = c(CRAN = ...))' > /root/.Rprofile
RUN Rscript -e 'install.packages(...)'

# Install Python packages using uv with specific versions for reproducibility.
RUN echo "pandas==2.3.3" > /tmp/requirements.txt && \
    echo "scikit-learn==1.7.2" >> /tmp/requirements.txt && \
    # ... more packages ...

RUN uv pip install --no-cache -r /tmp/requirements.txt && \
    rm /tmp/requirements.txt

# Install specific versions of Julia packages for reproducibility
RUN julia -e 'using Pkg; \
    Pkg.add(name="Arrow", version="2.8.0"); \
    # ... more packages ...'
```
:::



```{=tex}
\begin{CodeInput}
# Add R repository and install specific version
RUN apt-get update && apt-get install -y software-properties-common
RUN add-apt-repository ppa:...
RUN curl -L https://rig.r-pkg.org/... | sh
RUN rig add 4.5.1

# Install Python with uv
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
RUN uv python install 3.13

# Download and extract Julia
RUN curl -fsSL https://julialang-s3.julialang.org/... -o julia.tar.gz
RUN tar -xzf julia.tar.gz -C /opt/

# Install packages for each language separately
RUN echo 'options(repos = c(CRAN = ...))' > /root/.Rprofile
RUN Rscript -e 'install.packages(...)'

# Install Python packages using uv with specific versions for reproducibility.
RUN echo "pandas==2.3.3" > /tmp/requirements.txt && \
    echo "scikit-learn==1.7.2" >> /tmp/requirements.txt && \
    # ... more packages ...

RUN uv pip install --no-cache -r /tmp/requirements.txt && \
    rm /tmp/requirements.txt

# Install specific versions of Julia packages for reproducibility
RUN julia -e 'using Pkg; \
    Pkg.add(name="Arrow", version="2.8.0"); \
\end{CodeInput}
```

This procedural philosophy extends from environment definition to workflow
orchestration. A `Makefile` defines the pipeline as a directed acyclic graph of
file dependencies. While robust, this model requires the researcher to manually
encode the dependency graph, tightly coupling the analytical logic to specific
file paths. Furthermore, since `make` is agnostic to the content of these files,
it provides no mechanism to enforce type or schema consistency between steps.

A direct consequence of this file-based orchestration model is the proliferation
of wrapper scripts. Each step in the `Makefile` invokes a script containing a
significant amount of procedural boilerplate unrelated to the scientific logic.
This includes parsing command-line arguments, handling data serialization and
deserialization, and managing file paths. The core analytical logic is thus
obscured by necessary but extraneous scaffolding.

The cumulative effect is a system of high structural complexity. The traditional
implementation requires nine separate files to fully specify the environment,
workflow, and analysis. This includes the `Dockerfile`, `Makefile`, three
wrapper scripts for orchestration, and three `functions` files containing the
core logic, in addition to the final report. The cognitive overhead is
substantial, as a user must comprehend the conventions and interactions of this
entire toolchain.

### A Unified Declarative Framework

The Nix-based framework inverts this imperative model. Instead of scripting the
procedural construction of the environment and workflow, the researcher declares
the desired end-state.

Environment specification is reduced to a single declarative statement in the
`gen-env.R` script. The researcher specifies *what* the environment should
contain, leaving the procedural implementation of *how* to build it entirely to
the Nix toolchain. The `gen-env.R` script replaces the 108-line `Dockerfile`,
abstracting away all system administration details. `rix` translates this
high-level specification into a formal Nix expression that deterministically
resolves all transitive dependencies for the complete polyglot environment.

::: {.content-hidden when-format="pdf"}
```r
library(rix)
rix(
  date = "2025-10-14",
  r_pkgs = c("ggplot2", "dplyr", "arrow"),
  jl_conf = list(jl_version = "lts", ...),
  py_conf = list(py_version = "3.13", ...),
  ...
)
```
:::

```{=tex}
\begin{CodeInput}
R> library(rix)
R> rix(
+   date = "2025-10-14",
+   r_pkgs = c("ggplot2", "dplyr", "arrow"),
+   jl_conf = list(jl_version = "lts", ...),
+   py_conf = list(py_version = "3.13", ...),
+   ...
+ )
\end{CodeInput}
```

Workflow orchestration undergoes a similar conceptual shift from procedural to
declarative. The `gen-pipeline.R` script defines the pipeline not as a set of
file rules, but as a graph of data objects. Dependencies are not manually
specified but are automatically inferred by `rixpress` from the functional
relationships in the code. This object-centric model renders the procedural
wrapper scripts unnecessary. The scientific logic is expressed as a set of pure
functions that accept data structures as inputs and return them as outputs, free
of any boilerplate for file I/O or command-line parsing.

::: {.content-hidden when-format="pdf"}
```python
# functions/functions.py
def train_and_predict_output(simulated_df: pd.DataFrame) -> pd.DataFrame:
  # ... scientific logic ...
  return format_results(predictions)
```
:::

```{=tex}
\begin{CodeInput}
# functions/functions.py
Py> def train_and_predict_output(simulated_df: pd.DataFrame) -> pd.DataFrame:
+     # ... scientific logic ...
+     return format_results(predictions)
\end{CodeInput}
```

This declarative approach significantly reduces system complexity. The entire
implementation requires only six files: two for the high-level environment and
pipeline declarations, three containing the pure scientific functions, and the
final report. The reduction is not merely quantitative; the remaining files are
conceptually simpler, containing only analytical logic and specifications, with
all procedural scaffolding abstracted away by the framework.

### Reproducibility Guarantees: Space and Time

The critical distinction between the two paradigms lies in the nature of their
reproducibility guarantees. The traditional stack, anchored by Docker, provides
strong **spatial reproducibility**—the ability to execute the analysis
identically across different machines. However, it is vulnerable to **temporal
drift**, a concept that distinguishes reproducibility across space from
consistency over time [@malka2024]. This temporal vulnerability arises from
several sources, including the resolution of mutable base image tags (e.g.,
`ubuntu:24.04`) and the potential for language-level package managers to resolve
dependencies differently over time.

The Nix-based approach is architected to solve this problem of temporal drift.
By pinning the entire software ecosystem to a single, immutable revision of the
`nixpkgs` repository, the framework provides strong guarantees of **temporal
reproducibility**. This claim is supported by large-scale empirical evidence; a
study by @malka2025, which rebuilt over 700,000 historical packages,
confirmed that Nix's functional model achieves a bit-for-bit reproducibility
rate exceeding 90% and a rebuildability rate over 99% across a multi-year
timeframe.

### Summary of Differences

| Dimension | Traditional Stack | Nix-Based Framework with rix and rixpress |
|-----------|------------------|---------------------|
| **Paradigm** | Imperative composition of specialized tools | Declarative specification in unified framework |
| **Environment setup** | Step-by-step shell commands | High-level specification |
| **Workflow definition** | File-based rules in Make syntax | Object-based graph in R syntax |
| **Scientific code** | Wrapped in CLI parsing and file I/O boilerplate | Pure functions with no plumbing |
| **File count** | 9 files (4 for orchestration and plumbing) | 6 files (2 for orchestration) |
| **Expertise required** | Linux system administration, Make syntax | R programming |
| **Spatial reproducibility** | Strong | Strong |
| **Temporal reproducibility** | Moderate (vulnerable to drift) | Strong |

This declarative rigor, however, is not without pragmatic costs. Our validation
on GitHub Actions shows that the imperative Docker-based runner completes the
entire pipeline in approximately three minutes, whereas the declarative
Nix-based runner takes around five minutes.^[To run the benchmark yourself,
fork the paper’s repository, enable GitHub Actions, and push a single change to
trigger the runs.] Nix’s deterministic dependency
resolution guarantees bit-for-bit reproducibility at the expense of initial
computation time, whereas the imperative system prioritizes immediacy over
long-term stability.

## Conclusion {#sec-conclusion}

Many tools exist to improve reproducibility, but [Nix]{.pkg} stands out because 
it deploys complete software environments closed under the “depends on” relation:
it installs not only a package but all of its transitive dependencies. 
This makes [Nix]{.pkg} uniquely suited for reproducible research.

Yet solving such a complex problem makes [Nix]{.pkg} itself complex, as it relies on a 
functional paradigm still likely unfamiliar to most researchers.
[rix]{.pkg} lowers this barrier for [R]{.proglang} users by providing a familiar interface and workflow. 
By declaratively specifying reproducible development shells with [Nix]{.pkg}, 
researchers can accommodate diverse use cases—from running analytical pipelines 
to developing interactive [shiny]{.pkg} applications or serving [plumber]{.pkg} APIs.

Furthermore, [rixpress]{.pkg} extends this declarative model to entire analysis pipelines. 
By leveraging [Nix]{.pkg} as a unified build automation system, [rixpress]{.pkg} enables polyglot 
workflows where each step runs in a hermetically sealed environment. 
This ensures both spatial and temporal reproducibility, efficient caching, and seamless 
orchestration of multi-language analyses. Crucially, it provides native polyglot support 
without the manual orchestration required by container-based approaches, 
making complex workflows accessible without systems administration expertise.

Adopting [Nix]{.pkg}-based workflows represents a conceptual shift from procedural 
scripting to declarative specification. A practical adoption pathway begins with using 
[rix]{.pkg} to generate reproducible environments for new projects while maintaining existing workflows. 
This allows gradual learning of [Nix]{.pkg} concepts without disrupting active research. 
As confidence grows, researchers can extend to [rixpress]{.pkg} for smaller analyses before tackling complex 
pipelines. Institutions can support adoption by offering workshops, maintaining binary caches, 
and designating local experts. While early projects require investment, later projects benefit from
reusable environment definitions and cumulative expertise. Ultimately, adoption depends on 
project scope, reproducibility needs, and institutional context—but the framework now exists 
for those who require robust, long-term reproducibility.

While [rix]{.pkg} and [rixpress]{.pkg} (and [ryxpress]{.pkg}) represent significant progress,
current limitations remain. Debugging complex [rixpress]{.pkg} pipelines is more challenging than
troubleshooting [targets]{.pkg} workflows, especially when failures occur deep in the [Nix]{.pkg} build process. 
Visualization capabilities, though useful, currently lack features such as automatic detection 
of outdated derivations. Cross-platform reproducibility remains generally strong but is constrained on macOS by 
proprietary framework dependencies. These challenges are active areas for development and community contribution. 
Researchers adopting these tools should do so with realistic expectations: they provide strong guarantees of spatial 
and temporal reproducibility and scalable polyglot workflows, but they require patience during the learning phase 
and may occasionally present challenges that simpler tools avoid.

## Acknowledgments {.unnumbered}

We thank the rOpenSci reviewers and contributors who provided valuable feedback
on the development of [rix]{.pkg} and [rixpress]{.pkg}. In particular, we are
grateful to David Watkins and Jacob Wujiciak-Jens for their reviews of
[rix]{.pkg}, and to William Landau and Anthony Martinez for their reviews of
[rixpress]{.pkg}. We also acknowledge the contributions of Richard J. Acton,
Jordi Rosell, Elio Campitelli, László Kupcsik, and Michael Heming for
[rix]{.pkg}. Their expertise and feedback greatly improved the quality and
usability of these packages. We would also like to thank Pol Dellaiera and Edvin
Syk for providing feedback on the manuscript.

## References {.unnumbered}

:::{#refs}

:::

## Appendix {#sec-appendix}

### Reproducing this paper

The source code of this paper is hosted on GitHub and can be found
at following
[link](https://github.com/b-rodrigues/rix_paper)^[https://github.com/b-rodrigues/rix_paper].
The paper can be easily compiled by running the following command:

::: {.content-hidden when-format="pdf"}
```bash
nix-shell --run "quarto render paper.qmd --to jss-pdf"
```
:::

```{=tex}
\begin{CodeInput}
$> nix-shell --run "quarto render paper.qmd --to jss-pdf"
\end{CodeInput}
```

The `default.nix` file defines the exact computational environment required to
compile the manuscript, ensuring full reproducibility of the build process. To
guarantee the reproducibility of the manuscript, it is automatically recompiled
via GitHub Actions upon each commit, using the same [Nix]{.pkg} environment. An
HTML version is additionally deployed to GitHub Pages, providing an accessible
format for viewing on smaller devices at this
[link](https://b-rodrigues.github.io/rix_paper/)^[[https://b-rodrigues.github.io/rix_paper/](https://b-rodrigues.github.io/rix_paper/)].
The PDF version of the manuscript can be found at
[link](https://b-rodrigues.github.io/rix_paper/paper.pdf)^[[https://b-rodrigues.github.io/rix_paper/paper.pdf](https://b-rodrigues.github.io/rix_paper/paper.pdf)].

### A Complete Polyglot Example with rixpress

This appendix provides a conceptual walk-through of the polyglot pipeline
example discussed in Section 5. The pipeline simulates a RBC model in
[Julia]{.proglang}, trains an [XGBoost]{.pkg} model in [Python]{.proglang},
visualises the results in [R]{.proglang}, and compiles a final report with
[Quarto]{.pkg}.

The complete, runnable source code for this example is available in the paper's
GitHub repository. A shell script, `run_polyglot_example.sh`, is provided to
automate the entire process from environment creation to final output, as
described at the end of this
section^[\url{https://github.com/b-rodrigues/rix_paper/blob/master/repro-script/run_polyglot_example.sh}].

#### Project Structure and Components

The project is organized into a set of scripts, each with a distinct
responsibility. The core logic is separated from the environment definition and
pipeline orchestration.

*   `functions/`: This directory contains the helper scripts with the core analytical code for each language.
    *   `functions.jl`: The [Julia]{.proglang} script for the economic simulation.
    *   `functions.py`: The [Python]{.proglang} script for the machine learning workflow.
    *   `functions.R`: The [R]{.proglang} script for data visualisation.
*   `gen-env.R`: An [R]{.proglang} script that defines the reproducible computational environment.
*   `gen-pipeline.R`: The main [R]{.proglang} script that defines and orchestrates the entire polyglot pipeline.

#### Step 1: The Environment Definition

The foundation of the project is the reproducible environment, defined in
`gen-env.R`. This script uses the `rix()` function to programmatically generate
a `default.nix` file. This file serves as a complete blueprint for the
computational environment, specifying:

*   The exact versions of [R]{.proglang}, [Python]{.proglang}, and
    [Julia]{.proglang}.
*   A list of required packages for each language (e.g., [ggplot2]{.pkg} for
  [R]{.proglang}, [xgboost]{.pkg} for [Python]{.proglang}, [DataFrames]{.pkg}
  for [Julia]{.proglang}).
*   System-level dependencies like [Quarto]{.pkg}.

Crucially, the entire environment is pinned to a specific date (`2025-10-14`),
ensuring that anyone who builds the environment, now or in the future, will get
the exact same software versions, guaranteeing reproducibility.

By dropping into a temporary shell using the following command:

::: {.content-hidden when-format="pdf"}
```bash
nix-shell -I \
  nixpkgs=https://github.com/rstats-on-nix/nixpkgs/tarball/2025-10-20 -p \
  R rPackages.rix
```
:::

```{=tex}
\begin{CodeInput}
$> nix-shell -I \
+  nixpkgs=https://github.com/rstats-on-nix/nixpkgs/tarball/2025-10-20 -p \
+  R rPackages.rix
\end{CodeInput}
```

it is possible to generate the `default.nix` by sourcing `gen-env.R`. Then, one
leaves this temporary shell, and builds the environment using the command
`nix-shell`, which also drop the user into the development shell.

#### Step 2: The Core Analytical Logic

The scientific logic for each stage of the pipeline is encapsulated in the
separate helper scripts within the `functions/` directory.

*   The RBC Model Simulation (`functions.jl`): This [Julia]{.proglang} script
  contains a single pure function that implements the state-space solution to
  the RBC model, based on @depaoli2009rbc. It takes the model's economic
  parameters as inputs and returns the simulated time-series data as a
  [DataFrame]{.pkg}.
*   The [XGBoost]{.pkg} Forecasting Model (`functions.py`): This
  [Python]{.proglang} script handles the complete machine learning workflow
  through a series of modular functions. Its responsibilities include feature
  engineering (creating lagged variables), splitting the data into training and
  testing sets, training the [XGBoost]{.pkg} model, and generating predictions.
*   The Visualisation (`functions.R`): This [R]{.proglang} script contains a
  function that uses [ggplot2]{.pkg} to create the final visualisation. It is
  designed to take the data frame of actual and predicted values produced by the
  [Python]{.proglang} script and generate a plot comparing the two series.

#### Step 3: Orchestrating the Pipeline

The entire workflow is defined and orchestrated by `gen-pipeline.R`. This script
acts as the master plan, using functions from the [rixpress]{.pkg} package to
define each computational step as a *derivation*. It declaratively outlines the
dependencies between steps:

1.  It begins by defining the RBC model parameters in [Julia]{.proglang}.
2.  It specifies that the RBC simulation in `functions.jl` depends on these
    parameters.
3.  It then defines the series of [Python]{.proglang} steps (feature
  preparation, training, prediction), making each one dependent on the output of
  the previous one. The first [Python]{.proglang} step is explicitly made
  dependent on the output from the [Julia]{.proglang} simulation.
  [rixpress]{.pkg} handles the passing of data between languages, in this case
  using the [Arrow]{.pkg} file format for efficiency.
4.  Finally, it defines the **R visualisation** step, which depends on the final
  predictions from the [Python]{.proglang} model, and a **Quarto rendering**
  step that depends on the generated plot.

When this script is run in the development shell (by executing
`source("gen-pipeline.R")`), [rixpress]{.pkg} translates the declared pipeline
into a master [Nix]{.proglang} expression that [Nix]{.pkg} can execute,
automatically handling the caching of results and re-running only the necessary
steps if a piece of code or data changes.

To build the pipeline from an interactive [Python]{.proglang} session, one would
execute the following lines:

::: {.content-hidden when-format="pdf"}
```python
from ryxpress import rxp_make
rxp_make()
```
:::

```{=tex}
\begin{CodeInput}
Python> from ryxpress import rxp_make
Python> rxp_make()
\end{CodeInput}
```

#### Running the Project

This entire example can be executed by running the `run_polyglot_example.sh`
script available in the root of the paper's repository.
This script automates the full process:
1.  It first executes `gen-env.R` inside a temporary [Nix]{.pkg} shell to build
    the `default.nix` file.
2.  It then executes `gen-pipeline.R` inside the newly defined environment. This
  triggers [Nix]{.pkg} to run the entire polyglot pipeline in the correct order.

Upon completion, the script will have generated all intermediate artefacts and
the final `readme.html` report found in the `pipeline-output` folder containing
the visualisation.
