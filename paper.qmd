---
title: "A Framework for Reproducible Data Science using Nix"
format:
    jss-pdf:
        keep-tex: true
    jss-html: default
author:
  - name: Bruno Rodrigues 
    affiliations:
      - name: Ministry of Research and Higher education, Luxembourg 
        department: Department of Statistics
        address: 18, Montée de la Pétrusse
        city: Luxembourg 
        country: Luxembourg 
        postal-code: 2327
      - Journal of Statistical Software
    orcid: 0000-0002-3211-3689
    email: bruno@brodrigues.co
    url: https://www.brodrigues.co 
abstract: |
  To create a reproducible analysis, it is not enough to write clean,
  well-documented and tested code. One must also ensure that all software
  dependencies are clearly declared and, ideally, provide a simple way to
  install them. Several tools exist for this purpose, such as the
  containerization solution [Docker]{.pkg}. A complete solution, however,
  requires not only managing the computational environment but also
  orchestrating the execution of the analysis itself in a robust and automated
  way. This paper presents a comprehensive framework based on the [Nix]{.pkg}
  package manager to solve both problems. We first introduce the [R]{.proglang}
  package [rix]{.pkg}, which simplifies the creation of reproducible
  environments with [Nix]{.pkg}. We then introduce [rixpress]{.pkg}, a second
  [R]{.proglang} package that leverages these environments to define and execute
  complex, multi-language analytical pipelines. Together, these tools provide a
  powerful, unified approach to achieving high standards of computational
  reproducibility.

keywords: [reproducibility, R, Nix]
keywords-formatted: [reproducibility, "[R]{.proglang}", "[Nix]{.pkg}"]
number-sections: true

bibliography: bibliography.bib
---

## Introduction: Reproducibility is also about software {#sec-intro}

@peng2011 introduced the concept of reproducibility as a *continuum*. At one end
lies the least reproducible state, where only a paper describing the study is
available. Reproducibility improves when authors share the original source code,
improves further when they include the underlying data, and reaches its highest
level when what Roger Peng called *linked and executable code and data* are
provided.

By *linked and executable code and data*, Peng referred to compiled source code
and runnable scripts. In this paper, we interpret this notion more broadly as
the *computational environment*: the complete set of software required to
execute an analysis. Here too, a continuum exists. At the minimal end, authors
might only name the main software used—say, the [R]{.proglang} programming
language. More careful authors might also specify the version of [R]{.proglang},
or list the additional packages and their versions. Rarely, however, do authors
specify the operating system on which the analysis was performed, even though
differences in operating systems can lead to divergent results when using the
same code and software versions, as shown by @neupane2019. It is even less
common for authors to provide step-by-step installation instructions for the
required software stack.

Even when such instructions are given, they often fail across different
platforms or versions of the same platform. This lack of portability not only
hinders reproducibility but also complicates everyday research workflows.
Researchers working across multiple machines must be able to recreate the same
environment consistently, and collaborators must share identical computational
setups to avoid inconsistencies.

Finally, once the execution environment is correctly configured, additional
clarity is needed on how to *run* the project itself. Which packages should be
loaded first? Which scripts should be executed, and in what order? Without clear
documentation or a logically organized project structure, these operational
details become yet another barrier to reproducibility.

A range of tools now exist to help researchers approach the gold standard of
full reproducibility, or to consistently deploy the same development environment
across multiple machines. Let us first consider the most basic step in this
process: listing the software used. In [R]{.proglang}, the `sessionInfo()`
function provides a concise summary of the software environment, including the R
version, platform details, and all loaded packages. Its output can be saved to a
file and included as part of a study’s reproducibility record. Below is an
example output from `sessionInfo()`:

```{=tex}
\begin{CodeInput}
R> sessionInfo()
\end{CodeInput}
\begin{CodeOutput}
R version 4.3.2 (2023-10-31)
Platform: aarch64-unknown-linux-gnu (64-bit)
Running under: Ubuntu 22.04.3 LTS

Matrix products: default
BLAS:   /usr/lib/aarch64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/aarch64-linux-gnu/openblas-pthread/libopenblasp[...]

locale:
 LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 LC_ADDRESS=C               LC_TELEPHONE=C            
 LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

time zone: Etc/UTC
tzcode source: system (glibc)

attached base packages:
 stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 nnet_7.3-19  mgcv_1.9-0   nlme_3.1-163

loaded via a namespace (and not attached):
 compiler_4.3.2  Matrix_1.6-1.1  tools_4.3.2     splines_4.3.2
 grid_4.3.2      lattice_0.21-9 
\end{CodeOutput}
```

When an author includes this information, others attempting to reproduce the
study (future readers, collaborators, or even the author at a later
time) can easily see which version of [R]{.proglang} and which packages (with
their versions) were used. However, reproducing the environment still requires
manually installing the correct package versions. This can be difficult,
especially when packages depend on system-level libraries. For example, the
[nloptr]{.pkg} package requires a precompiled [nlopt]{.proglang} binary, or
alternatively, [cmake]{.proglang} when building from source on Linux or macOS.

A more robust approach than simply listing package versions is to use the
[renv]{.pkg} package. [renv]{.pkg} captures the project’s software state and
writes it to a `renv.lock` file, which includes the exact versions of
[R]{.proglang} and all required packages. This lockfile serves as a blueprint
for restoring the environment automatically, ensuring that others can recreate
the same setup with minimal effort. Below is an example of an `renv.lock` file:

```{=tex}
\begin{Code}
{
  "R": {
    "Version": "4.2.2",
    "Repositories": [
      {
        "Name": "CRAN",
        "URL": "https://packagemanager.rstudio.com/all/latest"
      }
    ]
  },
  "Packages": {
    "MASS": {
      "Package": "MASS",
      "Version": "7.3-58.1",
      "Source": "Repository",
      "Repository": "CRAN",
      "Hash": "762e1804143a332333c054759f89a706",
      "Requirements": []
    },
    "Matrix": {
      "Package": "Matrix",
      "Version": "1.5-1",
      "Source": "Repository",
      "Repository": "CRAN",
      "Hash": "539dc0c0c05636812f1080f473d2c177",
      "Requirements": [
        "lattice"
      ]
    }
    
    ... lines below omitted ...
  }
}
\end{Code}
```

This lockfile lists each package alongside its version and the repository from
which it was downloaded. Creating it requires only a single command:
`renv::init()`. Others can then reproduce the same package library by running
`renv::restore()`, which installs the exact package versions in an isolated,
project-specific library that does not interfere with the user’s global R
library.

However, [renv]{.pkg} does not restore the version of [R]{.proglang} itself.
Installing the correct R version must therefore be done separately. Similarly,
`renv` does not handle system-level dependencies such as [cmake]{.proglang},
which is required by [nloptr]{.pkg} on Linux and macOS. These dependencies must
be managed manually or through other tools. Further details are discussed in the
[renv]{.pkg} documentation: \url{https://rstudio.github.io/renv/articles/renv.html#caveats}.

Before continuing, it should be noted that other packages exist which provide
similar functionality to [renv]{.pkg}: there is [groundhog]{.pkg} by
@simonsohn2023 which makes it rather easy to install packages as they were on
CRAN at a given date. For example, the code snippet below installs the
[purrr]{.pkg} and [ggplot2]{.pkg} packages as they were on April 4th, 2017:

```{=tex}
\begin{CodeInput}
R> groundhog.library("
+   library('purrr')
+   library('ggplot2')",
+   "2017-10-04",
+   tolerate.R.version = "4.2.2")
\end{CodeInput}
```

Packages installed with [groundhog]{.pkg} are placed in a project-specific
library, ensuring they do not interfere with other versions of the same packages
used in different projects. Since [groundhog]{.pkg} does not install
[R]{.proglang} itself, users must either install the required R version manually
or use the `tolerate.R.version` argument, as illustrated in the example above.
Without this argument, [groundhog]{.pkg} will not proceed with package
installation if the R version does not match the expected one.

An alternative to [renv]{.pkg} is [rang]{.pkg}, developed by @chan2023, which
similarly allows users to install packages as they existed on a specific date.
Another approach is to use the Posit Package Manager, which provides dated
snapshots of CRAN. For instance, to ensure that packages are installed as they
were on June 30, 2023, one could include the following line in the `.Rprofile`
file:

```{=tex}
\begin{CodeInput}
R> options(repos =
+    c(REPO_NAME =
+      "https://packagemanager.posit.co/cran/__linux__/jammy/2023-06-30"
+    )
+  )
\end{CodeInput}
```

The `.Rprofile` file is read by [R]{.proglang} at the start of each new session,
meaning that every call to `install.packages()` will install packages from the
specified snapshot mirror. However, unless users explicitly manage separate
libraries for different projects, the Posit Package Manager will install all
packages for all projects as they existed on that snapshot date.

The next step toward the gold standard of reproducibility is ensuring not only
that the correct package versions are installed, but also that the correct
version of [R]{.proglang} itself is used. While this can be done manually,
dedicated tools make the process more efficient. One such tool is [rig]{.pkg},
developed by the [R]{.proglang} Infrastructure Team [-@rlib2023]. [rig]{.pkg}
simplifies installing and managing multiple R versions, allowing users to match
the exact version needed for a given analysis. Once the correct R version is
installed, one can then use tools such as [renv]{.pkg}, [groundhog]{.pkg}, or
[rang]{.pkg} to recreate the associated package library.

However, this multi-step process remains manual, error-prone, and
time-consuming, highlighting the need for more comprehensive and automated
solutions to achieve full computational reproducibility.

The final tool for achieving the gold standard of reproducibility is to bundle
the exact version of [R]{.proglang} and its packages within a [Docker]{.pkg}
image. [Docker]{.pkg} is a containerization platform that allows packaging a
*data product* together with all its dependencies into a self-contained image. A
statistical analysis can be viewed as such a data product, requiring a specific
set of software dependencies to run. *Dockerizing* an analysis involves building
an image that installs these dependencies, often using the tools mentioned
earlier, and includes the analysis scripts and data (but data can also be
dynamically made available at run-time, to avoid having to copy it inside the
image). The steps to create this image are specified in a simple text file
called a `Dockerfile`, which defines the structure and content of the
[Docker]{.pkg} image.

Once the image is built, the analysis can be executed inside a *container*,
which is a running instance of the image. Containers are typically run
non-interactively, allowing the complete computational environment to be
instantiated and executed with a single command.

The strength of [Docker]{.pkg} lies not only in its ability to encapsulate the
correct versions of [R]{.proglang} and its packages but also in its inclusion of
system-level dependencies. Since [Docker]{.pkg} images are nearly complete Linux
systems, they naturally bundle libraries and tools required by certain R
packages (for example, [nloptr]{.pkg}’s dependency on the [nlopt]{.proglang}
library). This ensures that future replicators of a study have access to the
same software environment, including all necessary system components. Moreover,
these images can be easily shared, enabling seamless reproducibility across
machines and collaborators.

The Rocker project, introduced by @boettiger2017, provides a collection of
pre-built [Docker]{.pkg} images for the [R]{.proglang} community. These images
come with specific R versions and, in some cases, preinstalled packages, making
them convenient base images for building reproducible analysis environments
without starting from scratch.

Despite its strengths, [Docker]{.pkg} can be cumbersome for interactive use.
Although containers can be modified at runtime, any changes are lost once the
container stops unless they are explicitly saved. Running graphical applications
from containers is also possible, but it is generally difficult to configure and
tends to work reliably only on Linux systems. For this reason, [Docker]{.pkg} is
most often used to run web-based applications (such as integrated development
environments, IDEs, like the web version of RStudio) inside containers, offering
a reproducible yet flexible environment for interactive work.

Some IDEs provide built-in support for executing code directly within a running
container, allowing a more seamless workflow. However, these setups typically
require additional configuration, and many general-purpose IDEs are not well
optimized for statistical or data science workflows (though Positron may soon
change that).

A common practice is to conduct a study interactively using a standard
installation of [R]{.proglang} and its packages, and only after completing the
analysis, create a `Dockerfile` to allow others to reproduce the results easily.
For instance, the [dockerfiler]{.pkg} package [@dockerfiler] can automatically
generate a `Dockerfile` from an existing `renv.lock` file, simplifying this
process. While this approach enables post-hoc reproducibility, it does not solve
the challenge of consistently deploying the same environment across multiple
machines as co-authors are working on the project. Ideally, analyses would be
developed directly within a containerized environment from the start, ensuring
that the computational setup is reproducible throughout the project lifecycle.

Another challenge of using [Docker]{.pkg} is that its images are effectively
minimal Linux systems, so familiarity with Linux is highly recommended for
writing reliable and efficient `Dockerfile`s. To guarantee that a build
consistently produces the same image, the `Dockerfile` should reference specific
image *digests* rather than general *tags*. In practice, however, tagged
versions are more commonly used, which can introduce potential reproducibility
issues. [dockerfiler]{.pkg} is an R package that lowers the
barrier to entry by automating parts of the `Dockerfile` creation process, but
they cannot fully replace the need for basic Linux knowledge.

A final step toward the gold standard of reproducibility is to automate the
analysis execution using a build system such as [Make]{.pkg}, rather than
relying on ad hoc script execution. Build automation tools enable analyses to be
defined as a series of reproducible, well-ordered steps that can be executed
consistently. Within [R]{.proglang}, a modern and powerful alternative is the
[targets]{.pkg} package by @landau2021, which provides declarative workflows for
reproducible data analysis.

@mcdermott2021 provides a clear example of a scientific study that achieved the
gold standard of reproducibility. The author created an accompanying GitHub
repository^[https://github.com/grantmcdermott/skeptic-priors] containing
detailed instructions to install the required software and execute the analysis.
Examining this repository reveals the use of multiple tools to capture and share
the computational environment:

- Packages and their versions were recorded in an `renv.lock` file;
- A `Makefile` was used to automate the full analysis and compile the paper;
- A `Dockerfile` provided a complete computational environment, including the
  correct version of [R]{.proglang}, enabling easy execution of the entire
  workflow.

Reaching this gold standard, however, is resource-intensive. It requires
learning a tool for managing package versions, mastering [Docker]{.pkg} for the
programming language and other software dependencies, and using a build
automation tool. Importantly, these considerations are not unique to
[R]{.proglang}: similar approaches are needed for [Python]{.proglang} or any
other programming language, and complexity further increases if several
programming languages are needed for a single project. 

As an alternative, we present the [Nix]{.pkg} package manager, which supports
all major operating systems and emphasizes reproducible software installation
and building. [Nix]{.pkg} can replace [Docker]{.pkg}, [renv]{.pkg}, and even
build automation tools such as [Make]{.pkg}. To make [Nix]{.pkg} more accessible
for [R]{.proglang} users, we developed the [rix]{.pkg} and [rixpress]{.pkgs}
packages, which are introduced and discussed in this article.

## The Nix package manager {#sec-nix}

[Nix]{.pkg} is a package manager designed to install and build software in a
fully reproducible manner. As of this writing, it provides access to over
120000 packages, including nearly all of CRAN and Bioconductor. This allows
users to install not only [R]{.proglang} itself but also all packages required
for a given project. Although [Nix]{.pkg} is the package manager of the NixOS
Linux distribution, it can also be installed as a standalone tool on other Linux
distributions, macOS, and the Windows Subsystem for Linux^[Effectively treating
non-NixOS Linux distributions and Windows as equivalent platforms in practice.].

The advantage of using [Nix]{.pkg} to install [R]{.proglang} packages, instead
of the standard `install.packages()` function, is that [Nix]{.pkg} ensures all
dependencies of each package are installed, whether they are other
[R]{.proglang} packages or system-level libraries.

For example, the [xlsx]{.pkg} package requires [Java]{.proglang} to be
installed. On some systems, installing [Java]{.proglang} may be difficult or
even impossible manually. With [Nix]{.pkg}, however, the user only needs to
declare that [xlsx]{.pkg} is required for the project; [Nix]{.pkg} automatically
installs and configures [Java]{.proglang} as a dependency. This works because
the maintainers of the [R]{.proglang} language packages for [Nix]{.pkg} have
declared [Java]{.proglang} as a dependency of [xlsx]{.pkg}, allowing the process
to happen seamlessly from the end-user perspective.

[xlsx]{.pkg} can thus be referred to as a component closure, and quoting
@dolstra2004nix:

> The idea is to always deploy component closures: if we deploy a component,
> then we must also deploy its dependencies, their dependencies, and so on. That
> is, we must always deploy a set of components that is closed under the
> ''depends on'' relation. Since closures are selfcontained, they are the units
> of complete software deployment. After all, if a set of components is not
> closed, it is not safe to deploy, since using them might cause other
> components to be referenced that are missing on the target system.

But how does [Nix]{.pkg} achieve this reproducibility? Where do these packages,
or *closures*, come from? When installing a package with [Nix]{.pkg}, an
expression written in the [Nix]{.pkg} language is downloaded from the `nixpkgs`
GitHub repository and evaluated. These expressions define *derivations*, which
describe how to build a package: specifying its dependencies, the commands to
build and install it, and the resulting output. Typically, a derivation
downloads the source code, compiles it, and outputs a binary. Derivations are
highly flexible; by writing custom derivations, users can define and build
reproducible environments instead of a single program. The dependencies of each
derivation are themselves defined in other expressions and are automatically
installed if needed. In the case of [R]{.proglang}, maintainers declare the
appropriate dependencies to create fully self-contained component closures.

Why is installing software with [Nix]{.pkg} reproducible? Because the entire set
of [Nix]{.pkg} expressions is hosted on GitHub, it is possible to reference a
specific commit of `nixpkgs`, a process called *pinning a revision*. Pinning a
revision ensures that all packages installed by [Nix]{.pkg} are always the exact
same versions, regardless of when in the future they are built, since the
expressions downloaded correspond to that specific commit of `nixpkgs`.

Pinning is crucial, but it is not the only reason [Nix]{.pkg} supports
reproducibility. [Nix]{.pkg} is a functional package manager: it applies
principles from functional programming, disallowing side effects and global
variables, and ensuring that the output of a build is always the same given the
same inputs, regardless of time or location. A side note: while this functional
approach greatly enhances reproducibility, it can make writing derivations more
complex, particularly for packages that need to download assets during
installation (e.g., some Bioconductor packages like [musData]{.pkg}). However,
this concern is primarily for package maintainers, not end-users.

Additionally, [Nix]{.pkg} supports multiple versions, or *variants*, of a
package on the same system. Each variant has a unique identifier, allowing
multiple versions of [R]{.proglang} to coexist and ensuring the correct version
is used for each project. For a more technical discussion of [Nix]{.pkg}, we
refer to @dolstra2004nix.

With [Nix]{.pkg}, it is possible to effectively replace both [renv]{.pkg} and
[Docker]{.pkg} for [R]{.proglang} projects—or, in the case of
[Python]{.proglang}, to replace `requirements.txt` files and virtual
environments. [Nix]{.pkg} also supports building multi-language environments,
which can include [R]{.proglang} and [Python]{.proglang}, a \LaTeX
distribution or any other of the 120,000 available tools. This enables the
creation of a complete, project-specific, and reproducible environment that can
be used interactively or non-interactively. As long as the `nixpkgs` GitHub
repository remains online, the environment can be rebuilt in the future to rerun
analyses reliably.

However, [Nix]{.pkg} has a steep learning curve. It is a complex system that
comes with its own programming language, also called [Nix]{.proglang}, designed
to solve the challenging problem of declaratively defining how software is built
and configured. This ensures that installations are fully reproducible across
operating systems and hardware platforms. To make [Nix]{.pkg} more accessible to
[R]{.proglang} users that wish to use project-specific and reproducible
development environments, we developed the [rix]{.pkg} package.

## Reproducible development environments with Nix {#sec-repro-nix}

As mentioned, [Nix]{.pkg} expressions are written in the [Nix]{.proglang}
programming language, which is purely functional. Here is a simple example that
creates a shell environment containing version 4.3.1 of [R]{.proglang}:

```{=tex}
\begin{CodeInput}
let
  pkgs = import (fetchTarball
    "https://github.com/NixOS/nixpkgs/archive/976fa336.tar.gz"
  ) {};
  system_packages = builtins.attrValues {
    inherit (pkgs) R;
  };
in
  pkgs.mkShell {
    buildInputs = [ system_packages ];
    shellHook = "R --vanilla";
  }
\end{CodeInput}
```

In this expression, the `let` keyword is used to define variables. The variable
`pkgs` imports the set of packages from the `nixpkgs` repository at the
specified commit `976fa336`. The variable `system_packages` lists the packages
to include in the environment; in this case, it is just the [R]{.proglang}
programming language, along with all its dependencies and their transitive
dependencies. The `mkShell` function then creates a development shell with the
specified packages. The `shellHook` is set to `"R --vanilla"`, meaning that
entering the shell automatically starts [R]{.proglang} in vanilla mode, ignoring
any startup options.

This expression can be saved in a file called `default.nix`. The environment can
then be built on a system with [Nix]{.pkg} installed using the `nix-build`
command.^[For installing [Nix]{.pkg}, we recommend the Determinate Systems
installer: \url{https://determinate.systems/posts/determinate-nix-installer}].
Once the build completes, the user can enter the interactive shell with
`nix-shell`. This shell contains all the packages specified in `default.nix` and
can be used for development, similar to activating a virtual environment in the
[Python]{.proglang} ecosystem.

Writing [Nix]{.pkg} expressions can be challenging for users unfamiliar with the
[Nix]{.proglang} language. However, the ability to define a fully reproducible
development environment in a single text file and then rebuild it anywhere is
highly appealing. To lower the barrier to adoption of [Nix]{.pkg} for
reproducibility, we developed the [rix]{.pkg} package.

[rix]{.pkg} provides the `rix()` function, which simplifies generating
[Nix]{.pkg} expressions. It is available on CRAN and can be installed like any
other R package. Additionally, it can bootstrap an [R]{.proglang} development
environment on a system where [R]{.proglang} is not yet installed but
[Nix]{.pkg} is available. This can be done by running (inside of a terminal):

```{=tex}
\begin{CodeInput}
$> nix-shell -I \
  nixpkgs=https://github.com/rstats-on-nix/nixpkgs/archive/refs/heads/2025-10-17.tar.gz -p \
  R rPackages.rix
\end{CodeInput}
```

(the `-I` flag allows one to pass a specific revision of `nixpkgs`, ensuring
temporary shells are also reproducible).

This command opens a temporary [R]{.proglang} session with [rix]{.pkg}
available.^[`nix-shell -p` starts an interactive shell with the specified
packages.] From there, users can generate new [Nix]{.pkg} expressions for
building environments. For example, the following generates a `default.nix` file
that installs [R]{.proglang} 4.3.1 along with the [dplyr]{.pkg} and
[chronicler]{.pkg} packages:

```{=tex}
\begin{CodeInput}
R> library('rix')

R> rix(r_ver = "4.3.1",
+    r_pkgs = c("dplyr", "chronicler"),
+    project_path = ".",
+    overwrite = TRUE)
\end{CodeInput}
```

[rix]{.pkg} can also handle more complex setups:

```{=tex}
\begin{CodeInput}
R> rix(r_ver = "4.3.1",
+    r_pkgs = c("dplyr", "chronicler", "AER@1.2-8"),
+    system_pkgs = c("quarto", "git"),
+    tex_pkgs = c(
+          "amsmath",
+          "framed",
+          "fvextra",
+          "environ",
+          "fontawesome5",
+          "orcidlink",
+          "pdfcol",
+          "tcolorbox",
+          "tikzfill"
+    ),
+    git_pkgs = list(
+      list(
+        package_name = "rix",
+        repo_url = "https://github.com/b-rodrigues/rix/",
+        branch_name = "master",
+        commit = "ea92a88ecdfc2d74bdf1dde3e441d008521b1756"),
+      list(
+        package_name = "fusen",
+        repo_url = "https://github.com/ThinkR-open/fusen",
+        branch_name = "main",
+        commit = "d617172447d2947efb20ad6a4463742b8a5d79dc")
+    ),
+    ide = "positron",
+    project_path = ".",
+    overwrite = TRUE)
\end{CodeInput}
```

This call to `rix()` generates an environment that installs several
[R]{.proglang} packages (including [AER]{.pkg} version 1.2-8), several TeXLive
packages for \LaTeX document authoring, development versions of [rix]{.pkg} and
[fusen]{.pkg} from GitHub, and the Positron editor.

[rix]{.pkg} can generate [Nix]{.pkg} expressions even if [Nix]{.pkg} is not
installed on the system. This is useful for continuous integration and
continuous deployment (CI/CD) workflows on platforms such as GitHub Actions. For
instance, the repository containing the source code for this
article^[https://github.com/b-rodrigues/rix_paper] uses GitHub Actions to
compile the paper. Each time a push is made to the master branch, a runner
installs [Nix]{.pkg}, generates the environment from the hosted `default.nix`
file, and compiles the paper using [Quarto]{.pkg} within the reproducible
environment. This ensure that *exactly* the same environment is used on the
author's computer and on the CI/CD without any additional, platform-specific,
configuration.

Instead of first entering a [Nix]{.pkg} shell, it is also possible to run a
program directly from the environment:

```{=tex}
\begin{CodeInput}
cd /path/to/project/ && nix-shell default.nix --run "Rscript analysis.R"
\end{CodeInput}
```

This command runs `Rscript` and executes the `analysis.R` script, which in this
example should be located in the same directory as `default.nix`.

## The rstats-on-nix fork of nixpkgs {#sec-fork}

As explained earlier, [Nix]{.pkg} uses expressions from the `nixpkgs` GitHub
repository to build software. However, when generating expressions with
[rix]{.pkg}, our fork `rstats-on-nix/nixpkgs` is used instead.

Using a fork offers several advantages. First, it provides flexibility that the
official `nixpkgs` repository cannot always accommodate. [Nix]{.pkg} is
primarily the package manager for the NixOS Linux distribution, and governance
and technical choices made upstream can limit what [rix]{.pkg} aims to provide.

For instance, while [Nix]{.pkg} can theoretically support multiple versions (or
*variants*) of the same package, in practice maintainers cannot provide several
variants for all [R]{.proglang} packages, given the size of the ecosystem (over
20,000 CRAN and Bioconductor packages). This makes it difficult to install a
specific version of an [R]{.proglang} package not included in a particular
`nixpkgs` commit. With [rix]{.pkg}, users can install a specific package version
from source, e.g.:

```{=tex}
\begin{CodeInput}
R> rix(..., r_pkgs = "dplyr@1.0.7", ...)
\end{CodeInput}
```

However, installing from source might fail, especially if the package needs
to be compiled.

Additionally, updating the full [R]{.proglang} package set on [Nix]{.pkg} daily
is impractical. While CRAN and Bioconductor update daily, the [R]{.proglang}
packages in `nixpkgs` are only updated with new R releases. This limitation is
due to [Nix]{.pkg}’s governance as a Linux distribution package manager.

Our fork allows us to circumvent these limitations. For example, we provide a
daily snapshot of CRAN. Each day, the [R]{.proglang} package set is updated and
committed to a dated branch using GitHub Actions. Users can select a specific
date with:

```{=tex}
\begin{CodeInput}
R> rix(date = "2024-12-14", ...)
\end{CodeInput}
```

We strive to provide an available date per week: each Monday, a GitHub Action
tests popular packages on Linux and macOS, and only if all tests succeed is the
date added to the list of available dates in [rix]{.pkg}. This ensures users can
reliably install packages, and allows us to backport fixes if needed. For
example, when RStudio was temporarily broken due to a dependency issue
(`boost`), a pull request was submitted to the official `nixpkgs` repository. We
backported the fix to our fork, making RStudio available to users of our fork
earlier than upstream, as merging PRs in the official repository can take some
time.

We have backported fixes to our `nixpkgs` fork as far back as March 2019. The
process involves checking out a `nixpkgs` commit on the selected date, updating
the [R]{.proglang} package set using Posit CRAN and Bioconductor snapshots,
backporting fixes, and ensuring popular packages work on both x86-linux
(including WSL2) and aarch64-darwin (Apple Silicon). These changes are committed
to a dated branch in `rstats-on-nix/nixpkgs`. Users can see all available dates
with `rix::available_dates()`.

A drawback of forking `nixpkgs` is that backported packages are not included
upstream and thus are not prebuilt by Nix’s CI platform, Hydra. Users may need
to build many packages from source, which can be time-consuming. To mitigate
this, we provide a binary cache sponsored by [Cachix](https://www.cachix.org/),
complementing the public Nix cache. Instructions for using Cachix are in
[rix]{.pkg}’s documentation. Using the cache significantly speeds up
installations, as prebuilt packages are downloaded rather than compiled.

## Orchestrating the workflow with rixpress {#sec-rixpress}

Defining a reproducible environment with [rix]{.pkg} addresses the first major
challenge of reproducibility. The second challenge is reliably and efficiently
executing the analysis workflow within that environment, which is the role of
the [rixpress]{.pkg} package.

As mentioned in the introduction, a build automation tool like [targets]{.pkg}
is invaluable for managing complex analyses. It tracks dependencies between code
and data, caches results, and only recomputes steps that have changed. One can
run a [targets]{.pkg} pipeline inside a Nix environment to make it reproducible.
However, this approach has limitations: the entire pipeline must run in a single
environment, and orchestrating steps across different languages (e.g.,
[R]{.proglang} and [Python]{.proglang}) requires manual handling via packages
like [reticulate]{.pkg}.

[rixpress]{.pkg} overcomes these limitations by using Nix not just as a package
manager, but as the build automation engine itself. In a [rixpress]{.pkg}
pipeline, each step is defined as a Nix derivation, providing two key benefits:

  1. True Polyglot Pipelines: Each step can have its own Nix environment. A
     Python step can run in a pure Python environment, an [R]{.proglang} step in
     an [R]{.proglang} environment, and a Quarto rendering step in yet another,
     all within the same pipeline.
  2. Deep Reproducibility: Each step is a hermetically sealed Nix derivation
     whose output is cached in the Nix store based on the hash of all its
     inputs. All artifacts are direct children of the computational environment
     (because the computational environment is actually a dependency of the
     artifacts), ensuring they are rebuilt if the environment changes, keeping
     environment and outputs always in sync.

To illustrate these capabilities with a realistic research task, we present a
polyglot pipeline that simulates a Real Business Cycle (RBC) model in
[Julia]{.proglang}, uses the resulting data to train an [XGBoost]{.pkg}
forecasting model in [Python]{.proglang}, visualizes the results in
[R]{.proglang} with [ggplot2]{.pkg}, and finally compiles a [Quarto]{.pkg}
report.

The user defines the pipeline in an [R]{.proglang} script (`gen-pipeline.R`)
using functions inspired by [targets]{.pkg}. The underlying logic for each step
is encapsulated in separate helper scripts (`functions.jl`, `functions.py`, and
`functions.R`), the full contents of which are detailed in the Appendix. The
high-level orchestration script demonstrates how `{rixpress}` defines a
granular, multi-step machine learning workflow that crosses language boundaries:

```{=tex}
\begin{CodeInput}
R> library('rixpress')

R> pipeline_steps <- list(
+  # STEP 0: Define RBC Model Parameters in Julia
+  rxp_jl(alpha, 0.3),
+  rxp_jl(beta, 1 / 1.01),
+  # ... (other parameters omitted for brevity) ...
+
+  # STEP 1: Julia - Simulate the RBC model
+  rxp_jl(
+    name = simulated_rbc_data,
+    expr = "simulate_rbc_model(alpha, beta, delta, rho, sigma, sigma_z)",
+    user_functions = "functions/functions.jl",
+    encoder = "arrow_write"
+  ),
+
+  # STEP 2.1: Python - Prepare features
+  rxp_py(
+    name = processed_data,
+    expr = "prepare_features(simulated_rbc_data)",
+    user_functions = "functions/functions.py",
+    decoder = "pyarrow.feather.read_feather"
+  ),
+
+  # STEP 2.2: Python - Split data (X_train, y_train, etc.)
+  rxp_py(name = X_train, expr = "get_X_train(processed_data)", ...),
+  # ... (other data splits omitted for brevity) ...
+
+  # STEP 2.3: Python - Train the XGBoost model
+  rxp_py(
+    name = trained_model,
+    expr = "train_model(X_train, y_train)",
+    user_functions = "functions/functions.py"
+  ),
+
+  # STEP 2.4: Python - Make predictions and format results
+  # ... (prediction and formatting steps omitted for brevity) ...
+  rxp_py(
+    name = final_predictions_df,
+    expr = "format_results(y_test, model_predictions)",
+    user_functions = "functions/functions.py",
+    encoder = "save_arrow"
+  ),
+
+  # STEP 3: R - Visualize the predictions
+  rxp_r(
+    name = output_plot,
+    expr = plot_predictions(final_predictions_df),
+    user_functions = "functions/functions.R",
+    decoder = arrow::read_feather
+  ),
+
+  # STEP 4: Quarto - Compile the final report
+  rxp_qmd(
+    name = final_report,
+    qmd_file = "readme.qmd"
+  )
+)

# Generate the 'pipeline.nix' file from the R list
R> rxp_populate(pipeline_steps, build = TRUE)
\end{CodeInput}
```

The `rxp_populate()` function translates this [R]{.proglang} list into a
`pipeline.nix` file, which declaratively defines the entire workflow. The
package can then generate a visual representation of the pipeline's directed
acyclic graph (DAG), as shown in Figure 1.

![Graphical representation of the polyglot pipeline. Green nodes are
[Julia]{.proglang}, yellow nodes are [Python]{.pkg}, and blue nodes are
[R]{.proglang} derivations.](dag-polyglot.png){#fig-dag}

To execute the pipeline, one can either set `build = TRUE` in `rxp_populate()`
or call `rxp_make()` separately. [Nix]{.pkg} executes each step in order,
building dependencies as needed. Outputs are cached, so subsequent runs only
recompute steps with changed inputs or code. This provides the efficiency of
[targets]{.pkg} with multi-language support and bit-for-bit reproducibility.
Artifacts can be inspected interactively in [R]{.proglang} using
`rxp_read("artifact_name")` or `rxp_load("artifact_name")`.

For Python users, a port called [ryxpress]{.pkg} allows building the same
pipelines and inspecting outputs from Python sessions. [rixpress]{.pkg} also
includes several additional features not covered here for brevity. It is also
possible to configure popular IDEs to work interactively and seamlessly with
both [rix]{.pkg} and [rixpress]{.pkg}, enabling a smooth, reproducible workflow
from within the development environment. Detailed setup instructions are
provided in the vignettes of both packages.

## Conclusion {#sec-conclusion}

Many tools exist to improve reproducibility, but [Nix]{.pkg} stands out because
it deploys complete software environments *closed under the "depends on"
relation*: it installs not only a package, but all its dependencies and their
dependencies. This makes [Nix]{.pkg} particularly powerful for reproducible
research.

However, solving such a complex problem makes [Nix]{.pkg} a complex tool. With
[rix]{.pkg}, we aim to make [Nix]{.pkg} more accessible to [R]{.proglang} users
by providing a familiar interface and workflow. By building reproducible
development shells with [Nix]{.pkg}, researchers can accommodate a wide range of
use cases: running scripts and pipelines, developing interactive [shiny]{.pkg}
applications, or serving [plumber]{.pkg} APIs.

Furthermore, [rixpress]{.pkg} extends this reproducibility to entire analysis
pipelines. By leveraging Nix as a build automation engine, [rixpress]{.pkg}
allows polyglot workflows where each step runs in its own hermetically sealed
environment. This ensures deep reproducibility, efficient caching, and seamless
orchestration of multi-language analyses, making it easier to manage complex
projects with confidence that results can be reproduced exactly.

## Acknowledgments {.unnumbered}

We thank the rOpenSci reviewers and contributors who provided valuable feedback
on the development of [rix]{.pkg} and [rixpress]{.pkg}. In particular, we are
grateful to David Watkins and Jacob Wujiciak-Jens for their reviews of `{rix}`,
and to William Landau and Anthony Martinez for their reviews of `{rixpress}`. We
also acknowledge the contributions of Richard J. Acton, Jordi Rosell, Elio
Campitelli, László Kupcsik, and Michael Heming for `{rix}`. Their expertise and
feedback greatly improved the quality and usability of these packages.

## References {.unnumbered}

:::{#refs}

:::

## Appendix: A Complete Polyglot Example with rixpress {#sec-appendix}

This appendix provides the complete source code for the polyglot pipeline
example discussed in Section 5. The pipeline simulates a Real Business Cycle
(RBC) model in [Julia]{.proglang}, trains an [XGBoost]{.pkg} model in
[Python]{.proglang}, and visualizes the results in [R]{.proglang}.

### The Environment Definition (`gen-env.R`)

This script uses [rix]{.pkg} to define a `default.nix` file that contains all
necessary dependencies for [R]{.proglang}, [Julia]{.proglang}, and
[Python]{.proglang}.

```{=tex}
\begin{CodeInput}
R> # This script defines the polyglot environment our pipeline will run in.
R> library(rix)
R>
R> # Define the complete execution environment
R> rix(
+   # Pin the environment to a specific date for reproducibility.
+   date = "2025-10-14",
+
+   # R Packages
+   r_pkgs = c(
+     "ggplot2", "dplyr", "arrow", "rix", "rixpress", "quarto"
+   ),
+ 
+   # Julia Configuration
+   jl_conf = list(
+     jl_version = "lts",
+     jl_pkgs = c("Distributions", "DataFrames", "Arrow", "Random")
+   ),
+ 
+   # Python Configuration
+   py_conf = list(
+     py_version = "3.13",
+     py_pkgs = c("pandas", "scikit-learn", "xgboost", "pyarrow")
+   ),
+ 
+   project_path = ".",
+   overwrite = TRUE
+ )
\end{CodeInput}
```

### The RBC Model Simulation (`functions/functions.jl`)

This Julia script implements the state-space solution to the RBC model as a pure
function. The economic theory is based on @depaoli2009.

```{=tex}
\begin{Code}
# This script contains the helper functions for the Julia portion of the pipeline.
# It implements the correct, stable, and canonical log-linearized solution to
# a standard Real Business Cycle (RBC) model, based on the provided lecture slides.

# In an interactive session, you should uncomment the line below
# to load the required packages. rixpress handles this automatically.
#using LinearAlgebra, Distributions, DataFrames, Arrow, Random

# 1. Main Simulation Function

"""
    simulate_rbc_model(α, β, δ, ρ, σ, σ_z)

    Takes RBC model parameters as input, solves for the state-space representation
    using the method of undetermined coefficients (as per the slides), simulates
    the model for 250 quarters, and returns a DataFrame.
"""
function simulate_rbc_model(α, β, δ, ρ, σ, σ_z)

    # --- STEADY-STATE VALUES AND CONSTRUCTED PARAMETERS (Slides p. 16, 31) ---
    y_k = ((1/β) - 1 + δ) / α

    # --- SOLVE FOR POLICY FUNCTION COEFFICIENTS (Slides p. 19-22) ---
    # We solve for the coefficients of the policy functions:
    # c_hat_t = c_ck * k_hat_{t-1} + c_cz * z_hat_t
    # k_hat_t = c_kk * k_hat_{t-1} + c_kz * z_hat_t

    # Solve the quadratic equation for c_ck (coefficient of capital on consumption)
    # Based on Campbell (1994) and slide 22, we take the stable root.
    # a*x^2 + b*x + c = 0
    a_quad = y_k - δ
    b_quad = -( (1-β*(1-δ))*(y_k - δ) + y_k*(1+α) + 1 )
    c_quad = y_k * α

    # The stable solution is the smaller positive root
    c_ck = (-b_quad - sqrt(b_quad^2 - 4*a_quad*c_quad)) / (2*a_quad)

    # Now solve for the other coefficients based on c_ck
    c_kk = (y_k * α) / (y_k - δ - c_ck)
    c_cz = (y_k * (1 - c_kk * (1 - α) * β * ρ)) /
           ( (y_k - δ - c_ck) * (1 - β * ρ) + σ * (1 - c_ck) * (1 - β * ρ) )
    c_kz = (c_cz * (1 - c_ck)) / (y_k * (1 - α))

    # --- BUILD STATE-SPACE MATRICES FROM SOLVED COEFFICIENTS ---
    # State vector: s_t = [k_{t-1}, z_t]'
    # Transition:   s_t = T*s_{t-1} + R*ε_t

    T = [c_kk  c_kz * ρ
         0     ρ      ]

    R = [c_kz ; 1]

    # Observation Matrix for [output, consumption, investment]
    # y_hat_t = α*k_{t-1} + z_t
    # c_hat_t = c_ck*k_{t-1} + c_cz*z_t
    # i_hat_t = (i_y_ss)^-1 * (k_t - (1-δ)k_{t-1})
    #         = (i_y_ss)^-1 * ( (c_kk - (1-δ))k_{t-1} + c_kz*z_t )

    i_y_ss = δ * (α / ((1/β) - 1 + δ))

    C = [α      1
         c_ck   c_cz
         (c_kk - (1-δ))/i_y_ss   c_kz/i_y_ss]

    # --- SIMULATE THE MODEL ---
    Random.seed!(1234)
    n_periods = 250
    shocks = randn(n_periods) * σ_z

    states = zeros(2, n_periods)
    for t in 2:n_periods
        # The state is [k_{t-1}, z_t]'
        # To get the next state [k_t, z_{t+1}]', we first find k_t
        k_t = T[1,1]*states[1, t] + T[1,2]*states[2, t] + R[1]*shocks[t]
        z_tp1 = T[2,1]*states[1, t] + T[2,2]*states[2, t] + R[2]*shocks[t]

        states[1, t] = k_t # This is now k_t, which is "k_lag" for t+1
        states[2, t] = z_tp1 # This is now z_{t+1}
    end
    # Re-aligning states to be [k_{t-1}, z_t]
    k_lag = [0; states[1, 1:end-1]]
    z = [0; states[2, 1:end-1]]

    observables = C * [k_lag'; z']

    df = DataFrame(
        period = 1:n_periods,
        output = observables[1, :],
        consumption = observables[2, :],
        investment = observables[3, :],
        capital = k_lag,
        technology = z
    )

    return df
end

# 2. Encoder Function (Unchanged)
"""
    arrow_write(df::DataFrame, path::String)
    Encoder function to save a Julia DataFrame to an Arrow file.
"""
function arrow_write(df::DataFrame, path::String)
    Arrow.write(path, df)
end
\end{Code}
```

### The XGBoost Forecasting Model (`functions/functions.py`)

This Python script contains a series of modular functions to handle feature
engineering, data splitting, model training, and prediction.

```{=tex}
\begin{Code}
# This script contains modular helper functions for the Python portion of the pipeline.
# Each function performs a single, distinct task in the ML workflow.

# 1. Load required Python packages
# This is only needed if you run this script by hand,
# in which case, uncomment lines 8 to 11.
# With rixpress, the packages get loaded automatically.
#import pandas as pd
#import pyarrow.feather as feather
#from sklearn.model_selection import train_test_split
#import xgboost as xgb
# This script contains the helper functions for the Python portion of the pipeline.
# It defines the ML model training and prediction logic as a pure function.
# This script contains the helper functions for the Python portion of the pipeline.
# It defines the ML model training and prediction logic as a pure function.

# Step 1: Feature Engineering
def prepare_features(simulated_df: pd.DataFrame) -> pd.DataFrame:
    """Takes the raw simulated data and creates lagged features and the target variable."""
    df = simulated_df.copy()

    # Create lagged features
    for col in ['output', 'consumption', 'investment', 'capital', 'technology']:
        df[f'{col}_lag1'] = df[col].shift(1)

    # Drop the first row which now has NaNs
    df.dropna(inplace=True)

    return df

# Step 2: Data Splitting Functions
def get_X_train(processed_df: pd.DataFrame):
    """Gets the training features (X_train) from the processed data."""
    features = [col for col in processed_df.columns if '_lag1' in col]
    X = processed_df[features]
    train_size = int(0.75 * len(X))
    return X[:train_size]

def get_y_train(processed_df: pd.DataFrame):
    """Gets the training target (y_train) from the processed data."""
    y = processed_df['output']
    train_size = int(0.75 * len(y))
    return y[:train_size]

def get_X_test(processed_df: pd.DataFrame):
    """Gets the testing features (X_test) from the processed data."""
    features = [col for col in processed_df.columns if '_lag1' in col]
    X = processed_df[features]
    train_size = int(0.75 * len(X))
    return X[train_size:]

def get_y_test(processed_df: pd.DataFrame):
    """Gets the testing target (y_test) from the processed data."""
    y = processed_df['output']
    train_size = int(0.75 * len(y))
    return y[train_size:]

# Step 3: Model Training
def train_model(X_train: pd.DataFrame, y_train: pd.Series):
    """Initializes and trains the XGBoost model."""
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        random_state=42
    )
    model.fit(X_train, y_train)
    return model

# Step 4: Prediction
def make_predictions(model, X_test: pd.DataFrame):
    """Makes predictions on the test set using the trained model."""
    return model.predict(X_test)

# Step 5: Format Final Results
def format_results(y_test: pd.Series, predictions) -> pd.DataFrame:
    """Combines test data and predictions into a final DataFrame for R."""
    results_df = pd.DataFrame({
        'period': y_test.index,
        'actual_output': y_test.values,
        'predicted_output': predictions
    })
    return results_df

# Encoder Function (for saving the final output)
def save_arrow(df: pd.DataFrame, path: str):
    """Encoder function to save a pandas DataFrame to an Arrow file."""
    feather.write_feather(df, path)


\end{Code}
```

### The Visualization (`functions/functions.R`)

This R script uses [ggplot2]{.pkg} to create a plot comparing the model's output
against the XGBoost predictions.

```{=tex}
\begin{CodeInput}
# This script contains the helper functions for the R portion of the pipeline.
# It defines the visualization logic using ggplot2.

# 1. Load required R packages
# This is only needed if you run this script by hand,
# in which case, uncomment lines 8 and 9.
# With rixpress, the packages get loaded automatically.
#library(ggplot2)
#library(dplyr)

#-------------------------------------------------------------------------------
# 2. Main Function: Create the Visualization
#-------------------------------------------------------------------------------

#' Create a plot comparing actual vs. predicted output
#'
#' This function takes a data frame with actual and predicted time series data
#' and generates a ggplot visualization.
#'
#' @param predictions_df A data frame containing columns: 'period',
#'   'actual_output', and 'predicted_output'. This data frame will be the
#'   output of the Python XGBoost step.
#' @return A ggplot object.
#'
plot_predictions <- function(predictions_df) {
  # Create the plot object
  p <- ggplot(predictions_df, aes(x = period)) +

    # Add a line for the actual output from the RBC model simulation
    geom_line(
      aes(y = actual_output, color = "Actual (RBC Model)"),
      linewidth = 1
    ) +

    # Add a dashed line for the XGBoost model's predictions
    geom_line(
      aes(y = predicted_output, color = "Predicted (XGBoost)"),
      linetype = "dashed",
      linewidth = 1
    ) +

    # Define custom colors and legend labels
    scale_color_manual(
      name = "Series",
      values = c("Actual (RBC Model)" = "blue", "Predicted (XGBoost)" = "red")
    ) +

    # Add informative labels and a title
    labs(
      title = "XGBoost Prediction of RBC Model Output",
      subtitle = "Forecasting next-quarter output based on current-quarter economic variables",
      x = "Time (Quarters)",
      y = "Output (Log-deviations from steady state)"
    ) +

    # Use a clean theme
    theme_minimal() +

    # Improve legend position
    theme(legend.position = "bottom")

  # Return the final ggplot object
  return(p)
}
\end{CodeInput}
```
