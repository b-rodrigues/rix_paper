---
title: "Nix for Polyglot, Reproducible Data Science Workflows"
format:
    jss-pdf:
        keep-tex: true
    jss-html: default
author:
  - name: Bruno Rodrigues 
    affiliations:
      - name: Ministry of Research and Higher education, Luxembourg 
        department: Department of Statistics
        address: 18, Montée de la Pétrusse
        city: Luxembourg 
        country: Luxembourg 
        postal-code: 2327
      - Journal of Statistical Software
    orcid: 0000-0002-3211-3689
    email: bruno@brodrigues.co
    url: https://www.brodrigues.co 
abstract: |
  Reproducible analysis requires more than clean, well-documented code: 
  researchers must also manage software dependencies, computational environments, and workflow execution. 
  Existing tools like [Docker]{.pkg} and [renv]{.pkg} address parts of this challenge, but no single solution handles multi-language environments, 
  system dependencies, and pipeline orchestration together. I present a two [R]{.proglang} packages that leverage [Nix]{.pkg} to provide both reproducible polyglot 
  environments and declarative workflow orchestration. [rix]{.pkg} generates [Nix]{.pkg} expressions that define reproducible environments 
  spanning [R]{.proglang}, [Python]{.proglang}, [Julia]{.proglang}, and system dependencies. Building on these environments, [rixpress]{.pkg} orchestrates polyglot pipelines where each 
  computational step runs in its own hermetically sealed environment, with automatic caching and dependency tracking. 
  This approach enables researchers to develop analyses interactively while maintaining bit-for-bit reproducibility, supports 
  collaboration across heterogeneous computational environments, and ensures that analyses remain executable years into the future.

keywords: [reproducibility, R, Nix]
keywords-formatted: [reproducibility, "[R]{.proglang}", "[Nix]{.pkg}"]
number-sections: true

bibliography: bibliography.bib
---

## Introduction: Reproducibility is also about software {#sec-intro}

@peng2011 introduced the concept of reproducibility as a *continuum*. At one end
lies the least reproducible state, where only a paper describing the study is
available. Reproducibility improves when authors share the original source code,
improves further when they include the underlying data, and reaches its highest
level when what Roger Peng called *linked and executable code and data* are
provided.

By *linked and executable code and data*, Peng referred to compiled source code
and runnable scripts. In this paper, we interpret this notion more broadly as
the *computational environment*: the complete set of software required to
execute an analysis. Here too, a continuum exists. At the minimal end, authors
might only name the main software used—say, the [R]{.proglang} programming
language. More careful authors might also specify the version of [R]{.proglang},
or list the additional packages and their versions. Rarely, however, do authors
specify the operating system on which the analysis was performed, even though
differences in operating systems can lead to divergent results when using the
same code and software versions, as shown by @neupane2019. It is even less
common for authors to provide step-by-step installation instructions for the
required software stack.

Even when such instructions are given, they often fail across different
platforms or versions of the same platform. This lack of portability not only
hinders reproducibility but also complicates everyday research workflows.
Researchers working across multiple machines must be able to recreate the same
environment consistently, and collaborators must share identical computational
setups to avoid inconsistencies.

Finally, once the execution environment is correctly configured, additional
clarity is needed on how to *run* the project itself. Which packages should be
loaded first? Which scripts should be executed, and in what order? Without clear
documentation or automated orchestration, these operational details become yet
another barrier to reproducibility.

This paper focuses specifically on two critical but often overlooked aspects of
reproducibility: *computational environment management* and *workflow
orchestration*. We present a comprehensive framework that addresses both
challenges using the [Nix]{.pkg} package manager, making it accessible to
researchers through two [R]{.proglang} packages: [rix]{.pkg} and
[rixpress]{.pkg}. Before introducing our approach, we survey existing tools and
their limitations to contextualize our contribution.

### Existing approaches to environment management {#sec-existing}

A range of tools now exist to help researchers approach the gold standard of
full reproducibility, or to consistently deploy the same development environment
across multiple machines. Let us first consider the most basic step in this
process: listing the software used. In [R]{.proglang}, the `sessionInfo()`
function provides a concise summary of the software environment, including the R
version, platform details, and all loaded packages. Its output can be saved to a
file and included as part of a study's reproducibility record. Below is an
example output from `sessionInfo()`:

```{=tex}
\begin{CodeInput}
R> sessionInfo()
\end{CodeInput}
\begin{CodeOutput}
R version 4.3.2 (2023-10-31)
Platform: aarch64-unknown-linux-gnu (64-bit)
Running under: Ubuntu 22.04.3 LTS

Matrix products: default
BLAS:   /usr/lib/aarch64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/aarch64-linux-gnu/openblas-pthread/libopenblasp[...]

locale:
 LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 ...

attached base packages:
 stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 nnet_7.3-19  mgcv_1.9-0   nlme_3.1-163

loaded via a namespace (and not attached):
 compiler_4.3.2  Matrix_1.6-1.1  tools_4.3.2     splines_4.3.2
 grid_4.3.2      lattice_0.21-9 
\end{CodeOutput}
```

When an author includes this information, others attempting to reproduce the
study (future readers, collaborators, or even the author at a later
time) can easily see which version of [R]{.proglang} and which packages (with
their versions) were used. However, reproducing the environment still requires
manually installing the correct package versions—a process that becomes
particularly difficult when packages depend on system-level libraries. For
example, the [sf]{.pkg} package requires [GDAL]{.pkg}, [GEOS]{.pkg}, and [PROJ]{.pkg} system libraries.
Installing and configuring these dependencies varies substantially across
operating systems and can be prohibitively difficult on some platforms.

#### Package-level solutions: renv and alternatives

A more robust approach than simply listing package versions is to use the
[renv]{.pkg} package. [renv]{.pkg} captures the project's software state and
writes it to a `renv.lock` file, which includes the exact versions of
[R]{.proglang} and all required packages. This lockfile serves as a blueprint
for restoring the environment automatically, ensuring that others can recreate
the same setup with minimal effort. Below is an abbreviated example of an
`renv.lock` file:
```{=tex}
\begin{Code}
{
  "R": {
    "Version": "4.2.2",
    "Repositories": [
      {
        "Name": "CRAN",
        "URL": "https://packagemanager.rstudio.com/all/latest"
      }
    ]
  },
  "Packages": {
    "MASS": {
      "Package": "MASS",
      "Version": "7.3-58.1",
      "Source": "Repository",
      "Repository": "CRAN",
      "Hash": "762e1804143a332333c054759f89a706",
      "Requirements": []
    },
    "Matrix": {
      "Package": "Matrix",
      "Version": "1.5-1",
      "Source": "Repository",
      "Repository": "CRAN",
      "Hash": "539dc0c0c05636812f1080f473d2c177",
      "Requirements": ["lattice"]
    }
    ... additional packages omitted ...
  }
}
\end{Code}
```

This lockfile lists each package alongside its version and the repository from
which it was downloaded. Creating it requires only a single command:
`renv::init()`. Others can then reproduce the same package library by running
`renv::restore()`, which installs the exact package versions in an isolated,
project-specific library that does not interfere with the user's global R
library.

However, [renv]{.pkg} has important limitations. It does not restore the version
of [R]{.proglang} itself—installing the correct R version must be done
separately using tools like [rig]{.pkg} [@rlib2023]. More critically,
[renv]{.pkg} does not handle system-level dependencies. If [sf]{.pkg} requires
[GDAL]{.pkg} version 3.0 but the system has version 2.4 installed, [renv]{.pkg} cannot
resolve this conflict. Users must manually install system libraries, and the
process differs across operating systems. Further details are discussed in the
[renv]{.pkg} documentation:
\url{https://rstudio.github.io/renv/articles/renv.html#caveats}.

Other packages provide similar functionality with different trade-offs.
[groundhog]{.pkg} by @simonsohn2023 makes it easy to install packages as they
existed on CRAN at a given date:
```{=tex}
\begin{CodeInput}
R> groundhog.library("
+   library('purrr')
+   library('ggplot2')",
+   "2017-10-04",
+   tolerate.R.version = "4.2.2")
\end{CodeInput}
```

[groundhog]{.pkg} places packages in project-specific libraries but does not
install [R]{.proglang} itself. Without the `tolerate.R.version` argument, it
will refuse to proceed if the R version does not match expectations.
[rang]{.pkg}, developed by @chan2023, offers similar date-based package
installation. The Posit Package Manager provides another approach through dated
CRAN snapshots, configured via the `.Rprofile` file:

```{=tex}
\begin{CodeInput}
R> options(repos =
+    c(REPO_NAME =
+      "https://packagemanager.posit.co/cran/__linux__/jammy/2023-06-30"
+    )
+  )
\end{CodeInput}
```

This approach installs all packages from the specified snapshot date, but unless
users explicitly manage separate libraries for different projects, all projects
will share the same package versions.

These tools represent significant progress in reproducibility, but they share a
common limitation: **none addresses system-level dependencies or supports
multi-language environments**. A project using [R]{.proglang},
[Python]{.proglang}, and system tools like [Quarto]{.pkg} or \LaTeX requires
coordinating multiple package managers ([renv]{.pkg} for R, virtual environments
for Python, manual installation for system tools), each with its own
configuration and potential for conflicts.

#### System-level solutions: Docker

The most comprehensive approach to reproducibility is containerization with
[Docker]{.pkg}. [Docker]{.pkg} packages a *data product* together with all its
dependencies—including the operating system, system libraries, programming
languages, and packages—into a self-contained image. A statistical analysis can
be viewed as such a data product. The steps to create an image are specified in
a `Dockerfile`, which defines the structure and content of the [Docker]{.pkg}
image.

Once the image is built, the analysis can be executed inside a *container*,
which is a running instance of the image. Containers are typically run
non-interactively, allowing the complete computational environment to be
instantiated and executed with a single command.

The strength of [Docker]{.pkg} lies in its ability to bundle not only the
correct versions of [R]{.proglang} and its packages but also system-level
dependencies. Since [Docker]{.pkg} images are effectively minimal Linux systems,
they naturally include libraries required by packages like [sf]{.pkg}. This
ensures that future replicators have access to the same software environment,
including all necessary system components. Moreover, these images can be easily
shared via registries like Docker Hub, enabling seamless reproducibility across
machines and collaborators.

The Rocker project, introduced by @boettiger2017, provides a collection of
pre-built [Docker]{.pkg} images for the [R]{.proglang} community. These images
come with specific R versions and, in some cases, preinstalled packages, making
them convenient base images for building reproducible analysis environments
without starting from scratch.

Despite its strengths, [Docker]{.pkg} presents several challenges for
statistical computing workflows:

1. Poor interactive support: While containers can be modified at runtime,
changes are lost when the container stops unless explicitly saved. Running
graphical applications like RStudio Desktop from containers requires complex X11
configuration that works reliably only on Linux. Instead, web-based IDEs (like RStudio
Server) are used instead, but come with the inherent limitations of web-based solutions.

2. Steep learning curve: [Docker]{.pkg} images are minimal Linux systems, so
writing reliable `Dockerfile`s requires familiarity with Linux system
administration. To ensure reproducibility, `Dockerfile`s should reference
specific image *digests* rather than *tags*, but this practice is rarely
followed. Packages like [dockerfiler]{.pkg} [@dockerfiler] help automate
`Dockerfile` creation but cannot eliminate the need for basic Linux knowledge.

3. Post-hoc reproducibility: A common workflow is to develop analyses
interactively using standard R installations, then create a `Dockerfile`
afterward to enable reproduction. While this achieves post-hoc reproducibility,
it fails to address the challenge of deploying consistent environments across
multiple machines *during* development, a critical need for collaborative
projects.

4. Single-language focus: While [Docker]{.pkg} can support multi-language
environments, orchestrating them requires manual coordination. There is no
standard way to specify that one step should run in a Python environment while
another runs in R.

### The workflow orchestration gap

Even with a perfectly reproducible environment, researchers face another
challenge: reliably executing the analysis workflow. Which scripts run first?
What are the dependencies between steps? Manual execution is error-prone and
poorly documented. Build automation tools like [Make]{.pkg} address this by
defining analyses as ordered, reproducible steps.

Within [R]{.proglang}, the [targets]{.pkg} package by @landau2021 provides a
modern, declarative approach to workflow management. [targets]{.pkg} tracks
dependencies between code and data, caches intermediate results, and only
recomputes steps affected by changes. This dramatically improves efficiency for
complex analyses.

However, [targets]{.pkg} operates within a single R session. While it can call
Python via [reticulate]{.pkg} or execute system commands, all steps share the
same computational environment. For truly polyglot pipelines (where different
steps may require incompatible dependencies or different language versions) this
limitation becomes problematic. One might run [targets]{.pkg} inside a
[Docker]{.pkg} container to ensure reproducibility, but this requires the entire
pipeline to use a single environment.

@mcdermott2021 provides an excellent example of achieving the gold standard of
reproducibility. The accompanying GitHub
repository^[https://github.com/grantmcdermott/skeptic-priors] demonstrates the
state of the art:

- Package versions recorded in an `renv.lock` file
- A `Makefile` automating the full analysis
- A `Dockerfile` providing a complete computational environment

However, achieving this required mastering multiple complex tools: [renv]{.pkg}
for package management, [Docker]{.pkg} for environment containerization, and
[Make]{.pkg} for workflow orchestration. Each tool has its own syntax, concepts,
and failure modes. The complexity compounds for multi-language projects—a Python
project would require learning virtual environments or conda, and coordinating R
and Python in a single analysis requires additional tooling.

### A unified approach with Nix

What researchers need is a tool that:

1. Manages complete environments including programming languages, packages,
and system dependencies;
2. Supports multiple languages natively, not through workarounds;
3. Works interactively without the complexity of containers;
4. Provides step-level isolation so different pipeline steps can use
different environments;
5. Ensures bit-for-bit reproducibility through deterministic builds;
6. Remains simple enough for researchers without systems administration
expertise.

The [Nix]{.pkg} package manager provides these capabilities. [Nix]{.pkg} ensures
reproducible software installation by deploying *component closures*—packages
bundled with all their dependencies and transitive dependencies [@dolstra2004nix].
This creates self-contained software environments that work identically across
machines and over time. [Nix]{.pkg} can replace [Docker]{.pkg} for environment
isolation, [renv]{.pkg} for package management, and even [Make]{.pkg} for
workflow orchestration—all within a single, unified framework.

However, [Nix]{.pkg} has a steep learning curve. It is a complex system with its
own purely functional programming language designed to declaratively define how
software is built and configured. While this functional approach ensures
reproducibility, it creates a significant barrier to adoption.

To make [Nix]{.pkg} accessible to researchers, I developed two [R]{.proglang}
packages:

- [rix]{.pkg} generates [Nix]{.pkg} expressions from intuitive R function
calls, eliminating the need to learn the [Nix]{.proglang} language. It handles
environment definition including [R]{.proglang}, [Python]{.proglang}, [Julia]{.proglang}, system tools, and their
dependencies.

- **[rixpress]{.pkg}** orchestrates polyglot analytical pipelines using
[Nix]{.pkg} as the build engine. Each pipeline step runs in its own
hermetically sealed environment with automatic caching and dependency tracking.
This enables true polyglot workflows where [R]{.proglang}, [Python]{.proglang} and [Julia]{.proglang} steps coexist
with different dependencies.

Together, these packages provide what existing tools cannot: a single framework
for managing reproducible, multi-language computational environments and
executing complex analytical workflows with step-level isolation. While
[Nix]{.pkg} remains complex for advanced use cases, [rix]{.pkg} and
[rixpress]{.pkg} abstract this complexity for common research workflows, making
deep reproducibility accessible without requiring systems administration
expertise.

The remainder of this paper proceeds as follows. Section 2 introduces the
[Nix]{.pkg} package manager and explains why its functional approach enables
reproducibility. Section 3 presents [rix]{.pkg} and demonstrates environment
definition for various use cases. Section 4 discusses our `rstats-on-nix` fork
of the official package repository and how it addresses practical limitations.
Section 5 introduces [rixpress]{.pkg} and demonstrates polyglot pipeline
orchestration through a complete example. Section 6 concludes with discussion of
appropriate use cases and future directions.

## The Nix package manager {#sec-nix}

[Nix]{.pkg} is a powerful, cross-platform package manager designed for installing 
and building software in a fully reproducible and reliable manner. Unlike traditional 
package managers, [Nix]{.pkg} emphasizes immutable infrastructure and a functional
approach, which significantly enhances the consistency of computational environments.
As of writing, the primary Nix package collection, `nixpkgs`, provides access to over 
120,000 packages, including nearly all of CRAN and Bioconductor, making it the largest,
and among the most up-to-date package repository according to Repology:

![nixpkgs is the largest and among the most up-to-date packages repository](repology.png){#fig-repology}

This extensive coverage allows users to install not only [R]{.proglang} itself but 
also all its required packages and system-level dependencies for any given project.

While [Nix]{.pkg} is the default package manager for the NixOS 
Linux distribution, it can be installed as a standalone tool on other Linux 
distributions, macOS, and effectively on Windows via the Windows Subsystem for 
Linux (WSL2), treating these as equivalent platforms in practice.

A key advantage of using [Nix]{.pkg} to install [R]{.proglang} packages, as 
opposed to the standard `install.packages()` function, lies in its comprehensive 
dependency management. [Nix]{.pkg} ensures that all dependencies of each package are
installed, irrespective of whether they are other [R]{.proglang} packages or 
underlying system libraries.

Consider, for example, the [sf]{.pkg} package, which is fundamental for spatial data analysis in [R]{.proglang}.
[sf]{.pkg} relies on several complex external system libraries such as [GDAL]{.proglang}, 
[GEOS]{.proglang}, and [PROJ]{.proglang}. Manually installing and configuring these libraries 
to be compatible with [sf]{.pkg} can be challenging and platform-specific. With [Nix]{.pkg}, 
however, the user merely needs to declare [sf]{.pkg} as a project requirement. 
[Nix]{.pkg} then automatically installs and configures all necessary system libraries
as transitive dependencies. This seamless process is possible because the maintainers of the 
[R]{.proglang} programming language for [Nix]{.pkg}
have declaratively specified these dependencies for [sf]{.pkg} in `nixpkgs`, allowing the entire software 
stack to be provisioned effortlessly from an end-user perspective.

This concept is central to [Nix]{.pkg}'s design, where packages are referred to as *component closures*. 
As @dolstra2004nix explains:

> The idea is to always deploy component closures: if we deploy a component,
> then we must also deploy its dependencies, their dependencies, and so on. That
> is, we must always deploy a set of components that is closed under the
> ''depends on'' relation. Since closures are selfcontained, they are the units
> of complete software deployment. After all, if a set of components is not
> closed, it is not safe to deploy, since using them might cause other
> components to be referenced that are missing on the target system.

The core of [Nix]{.pkg}'s reproducibility lies in its functional package management paradigm
and its unique approach to builds. When installing software with [Nix]{.pkg}, it evaluates
an *expression* written in the [Nix]{.proglang} language. These expressions define *derivations*,
which are declarative blueprints describing how to build a package. A derivation specifies all inputs: 
source code, build commands, and crucially, all direct and transitive dependencies. This declarative 
nature ensures that every build process is fully specified and independent of the environment where 
it's executed. Furthermore, all [Nix]{.pkg} expressions for official packages are hosted in the `nixpkgs`
GitHub repository. By referencing a specific commit of `nixpkgs` (a process known as *pinning a revision*) users 
guarantee that all packages installed are derived from the exact same set of build instructions.
This means that if an environment is built today using a pinned revision, it will produce the 
identical set of software versions and configurations if rebuilt tomorrow, next year, or on a 
different machine, as long as the `nixpkgs` repository remains accessible.

Pinning is crucial, but it is not the only reason [Nix]{.pkg} supports reproducibility.
[Nix]{.pkg} is a *functional* package manager in the programming sense: it treats software builds as pure functions.
Given the same inputs (source code, dependencies, build scripts), a [Nix]{.pkg} build will always produce the 
identical output, regardless of when or where it's executed. This is achieved by disallowing side effects and 
ensuring that builds operate within hermetic (isolated) environments, with no hidden dependencies on global system 
state. While this functional approach greatly enhances reproducibility, it can sometimes introduce complexity for
package maintainers, especially for software that needs to download external assets during installation (e.g., 
certain Bioconductor packages). For end-users, however, this complexity is largely abstracted away. Additionally, 
[Nix]{.pkg} supports multiple versions or *variants* of a package on the same system. Each package installed into
the "Nix store" (typically in `/nix/store`) has a unique identifier derived from a hash of all its inputs. This 
isolation ensures that projects always use their intended software versions, allowing multiple versions of
[R]{.proglang} to coexist and preventing "dependency hell" and global environment pollution.

For a more in-depth technical discussion of [Nix]{.pkg}'s design principles, we refer readers to @dolstra2004nix.

By leveraging these principles, [Nix]{.pkg} can effectively replace and integrate the functionalities of 
tools like [renv]{.pkg} and [Docker]{.pkg} for [R]{.proglang} projects, or `requirements.txt` files 
and virtual environments for [Python]{.proglang} projects. Furthermore, [Nix]{.pkg} excels at building
multi-language environments, which can seamlessly combine [R]{.proglang}, [Python]{.proglang}, [Julia]{.proglang},
a [LaTeX]{.proglang} distribution, or any of the numerous other tools available in `nixpkgs`. 
This enables the creation of a truly complete, project-specific, and deeply reproducible environment
that can be used interactively for development or non-interactively for automated analysis. 
As long as the `nixpkgs` repository (or a suitable fork, as discussed in Section @sec-fork) remains accessible,
the environment can be rebuilt reliably in the future.

[Nix]{.pkg} provides strong cross-platform support across major operating systems. It runs natively on Linux 
(x86_64) with full functionality and optimal integration. On macOS (Intel and Apple Silicon), however, 
reproducibility is less reliable due to dependencies on impure system frameworks and Xcode toolchains outside
the Nix store. As a result, macOS users may need to update project pins after system or Xcode upgrades to 
restore builds. On Windows, [Nix]{.pkg} works well through the Windows Subsystem for Linux (WSL2), though graphical
applications may need extra setup. Nix environments on WSL can be used interactively via VS Code or Positron 
for a smooth development experience.

While [Nix]{.pkg} is a powerful system, it comes with notable challenges. The steepest is its learning curve:
[Nix]{.pkg} uses its own declarative language (also called [Nix]{.proglang}), which can be difficult for 
those new to functional or declarative paradigms. Complexity becomes most apparent when writing custom 
derivations or troubleshooting intricate build issues. Build times can also be long, especially for large
environments, when pre-built binaries aren’t available. Additionally, because [Nix]{.pkg} stores all package
outputs immutably in the Nix store, disk usage tends to be higher than with traditional package managers. 
Finally, while `nixpkgs` is extensive, some new or niche packages may be missing or fail to build across platforms.

To make these capabilities more approachable for [R]{.proglang} users seeking reproducible, 
project-specific environments without learning the full [Nix]{.proglang} language, I created the 
[rix]{.pkg} and [rixpress]{.pkg} packages.

## Reproducible development environments with Nix {#sec-repro-nix}

As mentioned, [Nix]{.pkg} expressions are written in the [Nix]{.proglang}
programming language, which is purely functional. Here is a simple example that
creates a shell environment containing version 4.3.1 of [R]{.proglang}:

```{=tex}
\begin{CodeInput}
let
  pkgs = import (fetchTarball
    "https://github.com/NixOS/nixpkgs/archive/976fa336.tar.gz"
  ) {};
  system_packages = builtins.attrValues {
    inherit (pkgs) R;
  };
in
  pkgs.mkShell {
    buildInputs = [ system_packages ];
    shellHook = "R --vanilla";
  }
\end{CodeInput}
```

In this expression, the `let` keyword is used to define variables. The variable
`pkgs` imports the set of packages from the `nixpkgs` repository at the
specified commit `976fa336`. The variable `system_packages` lists the packages
to include in the environment; in this case, it is just the [R]{.proglang}
programming language, along with all its dependencies and their transitive
dependencies. The `mkShell` function then creates a development shell with the
specified packages. The `shellHook` is set to `"R --vanilla"`, meaning that
entering the shell automatically starts [R]{.proglang} in vanilla mode, ignoring
any startup options.

This expression can be saved in a file called `default.nix`. The environment can
then be built on a system with [Nix]{.pkg} installed using the `nix-build`
command.^[For installing [Nix]{.pkg}, we recommend the Determinate Systems
installer: \url{https://determinate.systems/posts/determinate-nix-installer}].
Once the build completes, the user can enter the interactive shell with
`nix-shell`. This shell contains all the packages specified in `default.nix` and
can be used for development, similar to activating a virtual environment in the
[Python]{.proglang} ecosystem.

Writing [Nix]{.pkg} expressions can be challenging for users unfamiliar with the
[Nix]{.proglang} language. However, the ability to define a fully reproducible
development environment in a single text file and then rebuild it anywhere is
highly appealing. To lower the barrier to adoption of [Nix]{.pkg} for
reproducibility, we developed the [rix]{.pkg} package.

[rix]{.pkg} provides the `rix()` function, which simplifies generating
[Nix]{.pkg} expressions. It is available on CRAN and can be installed like any
other R package. Additionally, it can bootstrap an [R]{.proglang} development
environment on a system where [R]{.proglang} is not yet installed but
[Nix]{.pkg} is available. This can be done by running (inside of a terminal):

```{=tex}
\begin{CodeInput}
$> nix-shell -I \
  nixpkgs=https://github.com/rstats-on-nix/nixpkgs/archive/refs/heads/2025-10-17.tar.gz -p \
  R rPackages.rix
\end{CodeInput}
```

(the `-I` flag allows one to pass a specific revision of `nixpkgs`, ensuring
temporary shells are also reproducible).

This command opens a temporary [R]{.proglang} session with [rix]{.pkg}
available.^[`nix-shell -p` starts an interactive shell with the specified
packages.] From there, users can generate new [Nix]{.pkg} expressions for
building environments. For example, the following generates a `default.nix` file
that installs [R]{.proglang} 4.3.1 along with the [dplyr]{.pkg} and
[chronicler]{.pkg} packages:

```{=tex}
\begin{CodeInput}
R> library('rix')

R> rix(r_ver = "4.3.1",
+    r_pkgs = c("dplyr", "chronicler"),
+    project_path = ".",
+    overwrite = TRUE)
\end{CodeInput}
```

[rix]{.pkg} can also handle more complex setups:

```{=tex}
\begin{CodeInput}
R> rix(r_ver = "4.3.1",
+    r_pkgs = c("dplyr", "chronicler", "AER@1.2-8"),
+    system_pkgs = c("quarto", "git"),
+    tex_pkgs = c(
+          "amsmath",
+          "framed",
+          "fvextra",
+          "environ",
+          "fontawesome5",
+          "orcidlink",
+          "pdfcol",
+          "tcolorbox",
+          "tikzfill"
+    ),
+    git_pkgs = list(
+      list(
+        package_name = "rix",
+        repo_url = "https://github.com/b-rodrigues/rix/",
+        branch_name = "master",
+        commit = "ea92a88ecdfc2d74bdf1dde3e441d008521b1756"),
+      list(
+        package_name = "fusen",
+        repo_url = "https://github.com/ThinkR-open/fusen",
+        branch_name = "main",
+        commit = "d617172447d2947efb20ad6a4463742b8a5d79dc")
+    ),
+    ide = "positron",
+    project_path = ".",
+    overwrite = TRUE)
\end{CodeInput}
```

This call to `rix()` generates an environment that installs several
[R]{.proglang} packages (including [AER]{.pkg} version 1.2-8), several TeXLive
packages for \LaTeX document authoring, development versions of [rix]{.pkg} and
[fusen]{.pkg} from GitHub, and the Positron editor.

[rix]{.pkg} can generate [Nix]{.pkg} expressions even if [Nix]{.pkg} is not
installed on the system. This is useful for continuous integration and
continuous deployment (CI/CD) workflows on platforms such as GitHub Actions. For
instance, the repository containing the source code for this
article^[https://github.com/b-rodrigues/rix_paper] uses GitHub Actions to
compile the paper. Each time a push is made to the master branch, a runner
installs [Nix]{.pkg}, generates the environment from the hosted `default.nix`
file, and compiles the paper using [Quarto]{.pkg} within the reproducible
environment. This ensure that *exactly* the same environment is used on the
author's computer and on the CI/CD without any additional, platform-specific,
configuration.

Instead of first entering a [Nix]{.pkg} shell, it is also possible to run a
program directly from the environment:

```{=tex}
\begin{CodeInput}
cd /path/to/project/ && nix-shell default.nix --run "Rscript analysis.R"
\end{CodeInput}
```

This command runs `Rscript` and executes the `analysis.R` script, which in this
example should be located in the same directory as `default.nix`.

## The rstats-on-nix fork of nixpkgs {#sec-fork}

As explained earlier, [Nix]{.pkg} uses expressions from the `nixpkgs` GitHub
repository to build software. However, when generating expressions with
[rix]{.pkg}, our fork `rstats-on-nix/nixpkgs` is used instead.

Using a fork offers several advantages. First, it provides flexibility that the
official `nixpkgs` repository cannot always accommodate. [Nix]{.pkg} is
primarily the package manager for the NixOS Linux distribution, and governance
and technical choices made upstream can limit what [rix]{.pkg} aims to provide.

For instance, while [Nix]{.pkg} can theoretically support multiple versions (or
*variants*) of the same package, in practice maintainers cannot provide several
variants for all [R]{.proglang} packages, given the size of the ecosystem (over
20,000 CRAN and Bioconductor packages). This makes it difficult to install a
specific version of an [R]{.proglang} package not included in a particular
`nixpkgs` commit. With [rix]{.pkg}, users can install a specific package version
from source, e.g.:

```{=tex}
\begin{CodeInput}
R> rix(..., r_pkgs = "dplyr@1.0.7", ...)
\end{CodeInput}
```

However, installing from source might fail, especially if the package needs
to be compiled.

Additionally, updating the full [R]{.proglang} package set on [Nix]{.pkg} daily
is impractical. While CRAN and Bioconductor update daily, the [R]{.proglang}
packages in `nixpkgs` are only updated with new R releases. This limitation is
due to [Nix]{.pkg}’s governance as a Linux distribution package manager.

Our fork allows us to circumvent these limitations. For example, we provide a
daily snapshot of CRAN. Each day, the [R]{.proglang} package set is updated and
committed to a dated branch using GitHub Actions. Users can select a specific
date with:

```{=tex}
\begin{CodeInput}
R> rix(date = "2024-12-14", ...)
\end{CodeInput}
```

We strive to provide an available date per week: each Monday, a GitHub Action
tests popular packages on Linux and macOS, and only if all tests succeed is the
date added to the list of available dates in [rix]{.pkg}. This ensures users can
reliably install packages, and allows us to backport fixes if needed. For
example, when RStudio was temporarily broken due to a dependency issue
(`boost`), a pull request was submitted to the official `nixpkgs` repository. We
backported the fix to our fork, making RStudio available to users of our fork
earlier than upstream, as merging PRs in the official repository can take some
time.

We have backported fixes to our `nixpkgs` fork as far back as March 2019. The
process involves checking out a `nixpkgs` commit on the selected date, updating
the [R]{.proglang} package set using Posit CRAN and Bioconductor snapshots,
backporting fixes, and ensuring popular packages work on both x86-linux
(including WSL2) and aarch64-darwin (Apple Silicon). These changes are committed
to a dated branch in `rstats-on-nix/nixpkgs`. Users can see all available dates
with `rix::available_dates()`.

A drawback of forking `nixpkgs` is that backported packages are not included
upstream and thus are not prebuilt by Nix’s CI platform, Hydra. Users may need
to build many packages from source, which can be time-consuming. To mitigate
this, we provide a binary cache sponsored by [Cachix](https://www.cachix.org/),
complementing the public Nix cache. Instructions for using Cachix are in
[rix]{.pkg}’s documentation. Using the cache significantly speeds up
installations, as prebuilt packages are downloaded rather than compiled.

## Orchestrating the workflow with rixpress {#sec-rixpress}

Defining a reproducible environment with [rix]{.pkg} addresses the first major
challenge of reproducibility. The second challenge is reliably and efficiently
executing the analysis workflow within that environment, which is the role of
the [rixpress]{.pkg} package.

As mentioned in the introduction, a build automation tool like [targets]{.pkg}
is invaluable for managing complex analyses. It tracks dependencies between code
and data, caches results, and only recomputes steps that have changed. One can
run a [targets]{.pkg} pipeline inside a Nix environment to make it reproducible.
However, this approach has limitations: the entire pipeline must run in a single
environment, and orchestrating steps across different languages (e.g.,
[R]{.proglang} and [Python]{.proglang}) requires manual handling via packages
like [reticulate]{.pkg}.

[rixpress]{.pkg} overcomes these limitations by using Nix not just as a package
manager, but as the build automation engine itself. In a [rixpress]{.pkg}
pipeline, each step is defined as a Nix derivation, providing two key benefits:

  1. True Polyglot Pipelines: Each step can have its own Nix environment. A
     Python step can run in a pure Python environment, an [R]{.proglang} step in
     an [R]{.proglang} environment, and a Quarto rendering step in yet another,
     all within the same pipeline.
  2. Deep Reproducibility: Each step is a hermetically sealed Nix derivation
     whose output is cached in the Nix store based on the hash of all its
     inputs. All artifacts are direct children of the computational environment
     (because the computational environment is actually a dependency of the
     artifacts), ensuring they are rebuilt if the environment changes, keeping
     environment and outputs always in sync.

To illustrate these capabilities with a realistic research task, we present a
polyglot pipeline that simulates a Real Business Cycle (RBC) model in
[Julia]{.proglang} [@bezanson2017julia], uses the resulting data to train an
[XGBoost]{.pkg} forecasting model in [Python]{.proglang}, visualizes the results
in [R]{.proglang} with [ggplot2]{.pkg}, and finally compiles a [Quarto]{.pkg}
report.

The user defines the pipeline in an [R]{.proglang} script (`gen-pipeline.R`)
using functions inspired by [targets]{.pkg}. The underlying logic for each step
is encapsulated in separate helper scripts (`functions.jl`, `functions.py`, and
`functions.R`), the full contents of which are detailed in the Appendix. The
high-level orchestration script demonstrates how `{rixpress}` defines a
granular, multi-step machine learning workflow that crosses language boundaries:

```{=tex}
\begin{CodeInput}
R> library('rixpress')

R> pipeline_steps <- list(
+  # STEP 0: Define RBC Model Parameters in Julia
+  rxp_jl(alpha, 0.3),
+  rxp_jl(beta, 1 / 1.01),
+  # ... (other parameters omitted for brevity) ...
+
+  # STEP 1: Julia - Simulate the RBC model
+  rxp_jl(
+    name = simulated_rbc_data,
+    expr = "simulate_rbc_model(alpha, beta, delta, rho, sigma, sigma_z)",
+    user_functions = "functions/functions.jl",
+    encoder = "arrow_write"
+  ),
+
+  # STEP 2.1: Python - Prepare features
+  rxp_py(
+    name = processed_data,
+    expr = "prepare_features(simulated_rbc_data)",
+    user_functions = "functions/functions.py",
+    decoder = "pyarrow.feather.read_feather"
+  ),
+
+  # STEP 2.2: Python - Split data (X_train, y_train, etc.)
+  rxp_py(name = X_train, expr = "get_X_train(processed_data)", ...),
+  # ... (other data splits omitted for brevity) ...
+
+  # STEP 2.3: Python - Train the XGBoost model
+  rxp_py(
+    name = trained_model,
+    expr = "train_model(X_train, y_train)",
+    user_functions = "functions/functions.py"
+  ),
+
+  # STEP 2.4: Python - Make predictions and format results
+  # ... (prediction and formatting steps omitted for brevity) ...
+  rxp_py(
+    name = final_predictions_df,
+    expr = "format_results(y_test, model_predictions)",
+    user_functions = "functions/functions.py",
+    encoder = "save_arrow"
+  ),
+
+  # STEP 3: R - Visualize the predictions
+  rxp_r(
+    name = output_plot,
+    expr = plot_predictions(final_predictions_df),
+    user_functions = "functions/functions.R",
+    decoder = arrow::read_feather
+  ),
+
+  # STEP 4: Quarto - Compile the final report
+  rxp_qmd(
+    name = final_report,
+    qmd_file = "readme.qmd"
+  )
+)

# Generate the 'pipeline.nix' file from the R list
R> rxp_populate(pipeline_steps, build = TRUE)
\end{CodeInput}
```

The `rxp_populate()` function translates this [R]{.proglang} list into a
`pipeline.nix` file, which declaratively defines the entire workflow. The
package can then generate a visual representation of the pipeline's directed
acyclic graph:

![Graphical representation of the polyglot pipeline. Purple nodes are
[Julia]{.proglang}, yellow nodes are [Python]{.pkg}, light blue nodes are
[R]{.proglang}, and dark blue nodes are Quarto derivations.](dag.png){#fig-dag}

Figure @fig-dag can be generated by calling `rxp_ggdag()`.

To execute the pipeline, one can either set `build = TRUE` in `rxp_populate()`
or call `rxp_make()` separately. [Nix]{.pkg} executes each step in order,
building dependencies as needed. Outputs are cached, so subsequent runs only
recompute steps with changed inputs or code. This provides the efficiency of
[targets]{.pkg} with multi-language support and bit-for-bit reproducibility.
Artifacts can be inspected interactively in [R]{.proglang} using
`rxp_read("artifact_name")` or `rxp_load("artifact_name")`.

For Python users, a port called [ryxpress]{.pkg} allows building the same
pipelines and inspecting outputs from Python sessions. [rixpress]{.pkg} also
includes several additional features not covered here for brevity. It is also
possible to configure popular IDEs to work interactively and seamlessly with
both [rix]{.pkg} and [rixpress]{.pkg}, enabling a smooth, reproducible workflow
from within the development environment. Detailed setup instructions are
provided in the vignettes of both packages.

## Conclusion {#sec-conclusion}

Many tools exist to improve reproducibility, but [Nix]{.pkg} stands out because
it deploys complete software environments *closed under the "depends on"
relation*: it installs not only a package, but all its dependencies and their
dependencies. This makes [Nix]{.pkg} particularly powerful for reproducible
research.

However, solving such a complex problem makes [Nix]{.pkg} a complex tool. With
[rix]{.pkg}, we aim to make [Nix]{.pkg} more accessible to [R]{.proglang} users
by providing a familiar interface and workflow. By building reproducible
development shells with [Nix]{.pkg}, researchers can accommodate a wide range of
use cases: running scripts and pipelines, developing interactive [shiny]{.pkg}
applications, or serving [plumber]{.pkg} APIs.

Furthermore, [rixpress]{.pkg} extends this reproducibility to entire analysis
pipelines. By leveraging Nix as a build automation engine, [rixpress]{.pkg}
allows polyglot workflows where each step runs in its own hermetically sealed
environment. This ensures deep reproducibility, efficient caching, and seamless
orchestration of multi-language analyses, making it easier to manage complex
projects with confidence that results can be reproduced exactly.

## Acknowledgments {.unnumbered}

We thank the rOpenSci reviewers and contributors who provided valuable feedback
on the development of [rix]{.pkg} and [rixpress]{.pkg}. In particular, we are
grateful to David Watkins and Jacob Wujiciak-Jens for their reviews of `{rix}`,
and to William Landau and Anthony Martinez for their reviews of `{rixpress}`. We
also acknowledge the contributions of Richard J. Acton, Jordi Rosell, Elio
Campitelli, László Kupcsik, and Michael Heming for `{rix}`. Their expertise and
feedback greatly improved the quality and usability of these packages.

## References {.unnumbered}

:::{#refs}

:::

## Appendix: A Complete Polyglot Example with rixpress {#sec-appendix}

This appendix provides the complete source code for the polyglot pipeline
example discussed in Section 5. The pipeline simulates a Real Business Cycle
(RBC) model in [Julia]{.proglang}, trains an [XGBoost]{.pkg} model in
[Python]{.proglang}, and visualizes the results in [R]{.proglang}.

### The Environment Definition

This code snippet uses [rix]{.pkg} to generate a `default.nix` file that contains all
necessary dependencies for [R]{.proglang}, [Julia]{.proglang}, and
[Python]{.proglang}. It is typically saved in a file called `gen-env.R`:

```{=tex}
\begin{CodeInput}
# This script defines the polyglot environment our pipeline will run in.
library(rix)

# Define the complete execution environment
rix(
 # Pin the environment to a specific date for reproducibility.
 date = "2025-10-14",

 # R Packages
 r_pkgs = c(
   "ggplot2", "dplyr", "arrow", "rix", "rixpress", "quarto"
 ),

 # Julia Configuration
 jl_conf = list(
   jl_version = "lts",
   jl_pkgs = c("Distributions", "DataFrames", "Arrow", "Random")
 ),

 # Python Configuration
 py_conf = list(
   py_version = "3.13",
   py_pkgs = c("pandas", "scikit-learn", "xgboost", "pyarrow")
 ),

 project_path = ".",
 overwrite = TRUE
)
\end{CodeInput}
```

### The RBC Model Simulation

This Julia script implements the state-space solution to the RBC model as a pure
function, and is saved in a subfolder of the project named `functions` and in a script
`functions.jl`. The economic theory is based on @depaoli2009rbc.

```{=tex}
\begin{Code}
# This script contains the helper functions for the Julia portion of the pipeline.
# It implements the correct, stable, and canonical log-linearized solution to
# a standard Real Business Cycle (RBC) model, based on the provided lecture slides.

# In an interactive session, you should uncomment the line below
# to load the required packages. rixpress handles this automatically.
#using LinearAlgebra, Distributions, DataFrames, Arrow, Random

# 1. Main Simulation Function

"""
    simulate_rbc_model(α, β, δ, ρ, σ, σ_z)

    Takes RBC model parameters as input, solves for the state-space representation
    using the method of undetermined coefficients (as per the slides), simulates
    the model for 250 quarters, and returns a DataFrame.
"""
function simulate_rbc_model(α, β, δ, ρ, σ, σ_z)

    # --- STEADY-STATE VALUES AND CONSTRUCTED PARAMETERS (Slides p. 16, 31) ---
    y_k = ((1/β) - 1 + δ) / α

    # --- SOLVE FOR POLICY FUNCTION COEFFICIENTS (Slides p. 19-22) ---
    # We solve for the coefficients of the policy functions:
    # c_hat_t = c_ck * k_hat_{t-1} + c_cz * z_hat_t
    # k_hat_t = c_kk * k_hat_{t-1} + c_kz * z_hat_t

    # Solve the quadratic equation for c_ck (coefficient of capital on consumption)
    # Based on Campbell (1994) and slide 22, we take the stable root.
    # a*x^2 + b*x + c = 0
    a_quad = y_k - δ
    b_quad = -( (1-β*(1-δ))*(y_k - δ) + y_k*(1+α) + 1 )
    c_quad = y_k * α

    # The stable solution is the smaller positive root
    c_ck = (-b_quad - sqrt(b_quad^2 - 4*a_quad*c_quad)) / (2*a_quad)

    # Now solve for the other coefficients based on c_ck
    c_kk = (y_k * α) / (y_k - δ - c_ck)
    c_cz = (y_k * (1 - c_kk * (1 - α) * β * ρ)) /
           ( (y_k - δ - c_ck) * (1 - β * ρ) + σ * (1 - c_ck) * (1 - β * ρ) )
    c_kz = (c_cz * (1 - c_ck)) / (y_k * (1 - α))

    # --- BUILD STATE-SPACE MATRICES FROM SOLVED COEFFICIENTS ---
    # State vector: s_t = [k_{t-1}, z_t]'
    # Transition:   s_t = T*s_{t-1} + R*ε_t

    T = [c_kk  c_kz * ρ
         0     ρ      ]

    R = [c_kz ; 1]

    # Observation Matrix for [output, consumption, investment]
    # y_hat_t = α*k_{t-1} + z_t
    # c_hat_t = c_ck*k_{t-1} + c_cz*z_t
    # i_hat_t = (i_y_ss)^-1 * (k_t - (1-δ)k_{t-1})
    #         = (i_y_ss)^-1 * ( (c_kk - (1-δ))k_{t-1} + c_kz*z_t )

    i_y_ss = δ * (α / ((1/β) - 1 + δ))

    C = [α      1
         c_ck   c_cz
         (c_kk - (1-δ))/i_y_ss   c_kz/i_y_ss]

    # --- SIMULATE THE MODEL ---
    Random.seed!(1234)
    n_periods = 250
    shocks = randn(n_periods) * σ_z

    states = zeros(2, n_periods)
    for t in 2:n_periods
        # The state is [k_{t-1}, z_t]'
        # To get the next state [k_t, z_{t+1}]', we first find k_t
        k_t = T[1,1]*states[1, t] + T[1,2]*states[2, t] + R[1]*shocks[t]
        z_tp1 = T[2,1]*states[1, t] + T[2,2]*states[2, t] + R[2]*shocks[t]

        states[1, t] = k_t # This is now k_t, which is "k_lag" for t+1
        states[2, t] = z_tp1 # This is now z_{t+1}
    end
    # Re-aligning states to be [k_{t-1}, z_t]
    k_lag = [0; states[1, 1:end-1]]
    z = [0; states[2, 1:end-1]]

    observables = C * [k_lag'; z']

    df = DataFrame(
        period = 1:n_periods,
        output = observables[1, :],
        consumption = observables[2, :],
        investment = observables[3, :],
        capital = k_lag,
        technology = z
    )

    return df
end

# 2. Encoder Function (Unchanged)
"""
    arrow_write(df::DataFrame, path::String)
    Encoder function to save a Julia DataFrame to an Arrow file.
"""
function arrow_write(df::DataFrame, path::String)
    Arrow.write(path, df)
end
\end{Code}
```

### The XGBoost Forecasting Model

This Python script contains a series of modular functions to handle feature
engineering, data splitting, model training, and prediction, and is saved in
`functions/functions.py`.

```{=tex}
\begin{Code}
# This script contains modular helper functions for the Python portion of the pipeline.
# Each function performs a single, distinct task in the ML workflow.

# 1. Load required Python packages
# This is only needed if you run this script by hand,
# in which case, uncomment lines 8 to 11.
# With rixpress, the packages get loaded automatically.
#import pandas as pd
#import pyarrow.feather as feather
#from sklearn.model_selection import train_test_split
#import xgboost as xgb
# This script contains the helper functions for the Python portion of the pipeline.
# It defines the ML model training and prediction logic as a pure function.
# This script contains the helper functions for the Python portion of the pipeline.
# It defines the ML model training and prediction logic as a pure function.

# Step 1: Feature Engineering
def prepare_features(simulated_df: pd.DataFrame) -> pd.DataFrame:
    """Takes the raw simulated data and creates lagged features and the target variable."""
    df = simulated_df.copy()

    # Create lagged features
    for col in ['output', 'consumption', 'investment', 'capital', 'technology']:
        df[f'{col}_lag1'] = df[col].shift(1)

    # Drop the first row which now has NaNs
    df.dropna(inplace=True)

    return df

# Step 2: Data Splitting Functions
def get_X_train(processed_df: pd.DataFrame):
    """Gets the training features (X_train) from the processed data."""
    features = [col for col in processed_df.columns if '_lag1' in col]
    X = processed_df[features]
    train_size = int(0.75 * len(X))
    return X[:train_size]

def get_y_train(processed_df: pd.DataFrame):
    """Gets the training target (y_train) from the processed data."""
    y = processed_df['output']
    train_size = int(0.75 * len(y))
    return y[:train_size]

def get_X_test(processed_df: pd.DataFrame):
    """Gets the testing features (X_test) from the processed data."""
    features = [col for col in processed_df.columns if '_lag1' in col]
    X = processed_df[features]
    train_size = int(0.75 * len(X))
    return X[train_size:]

def get_y_test(processed_df: pd.DataFrame):
    """Gets the testing target (y_test) from the processed data."""
    y = processed_df['output']
    train_size = int(0.75 * len(y))
    return y[train_size:]

# Step 3: Model Training
def train_model(X_train: pd.DataFrame, y_train: pd.Series):
    """Initializes and trains the XGBoost model."""
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        random_state=42
    )
    model.fit(X_train, y_train)
    return model

# Step 4: Prediction
def make_predictions(model, X_test: pd.DataFrame):
    """Makes predictions on the test set using the trained model."""
    return model.predict(X_test)

# Step 5: Format Final Results
def format_results(y_test: pd.Series, predictions) -> pd.DataFrame:
    """Combines test data and predictions into a final DataFrame for R."""
    results_df = pd.DataFrame({
        'period': y_test.index,
        'actual_output': y_test.values,
        'predicted_output': predictions
    })
    return results_df

# Encoder Function (for saving the final output)
def save_arrow(df: pd.DataFrame, path: str):
    """Encoder function to save a pandas DataFrame to an Arrow file."""
    feather.write_feather(df, path)


\end{Code}
```

### The Visualization

This R snippet uses [ggplot2]{.pkg} to create a plot comparing the model's output
against the XGBoost predictions and is saved in `functions/functions.R`.

```{=tex}
\begin{CodeInput}
# This script contains the helper functions for the R portion of the pipeline.
# It defines the visualization logic using ggplot2.

# 1. Load required R packages
# This is only needed if you run this script by hand,
# in which case, uncomment lines 8 and 9.
# With rixpress, the packages get loaded automatically.
#library(ggplot2)
#library(dplyr)

#-------------------------------------------------------------------------------
# 2. Main Function: Create the Visualization
#-------------------------------------------------------------------------------

#' Create a plot comparing actual vs. predicted output
#'
#' This function takes a data frame with actual and predicted time series data
#' and generates a ggplot visualization.
#'
#' @param predictions_df A data frame containing columns: 'period',
#'   'actual_output', and 'predicted_output'. This data frame will be the
#'   output of the Python XGBoost step.
#' @return A ggplot object.
#'
plot_predictions <- function(predictions_df) {
  # Create the plot object
  p <- ggplot(predictions_df, aes(x = period)) +

    # Add a line for the actual output from the RBC model simulation
    geom_line(
      aes(y = actual_output, color = "Actual (RBC Model)"),
      linewidth = 1
    ) +

    # Add a dashed line for the XGBoost model's predictions
    geom_line(
      aes(y = predicted_output, color = "Predicted (XGBoost)"),
      linetype = "dashed",
      linewidth = 1
    ) +

    # Define custom colors and legend labels
    scale_color_manual(
      name = "Series",
      values = c("Actual (RBC Model)" = "blue", "Predicted (XGBoost)" = "red")
    ) +

    # Add informative labels and a title
    labs(
      title = "XGBoost Prediction of RBC Model Output",
      subtitle = "Forecasting next-quarter output based on current-quarter economic variables",
      x = "Time (Quarters)",
      y = "Output (Log-deviations from steady state)"
    ) +

    # Use a clean theme
    theme_minimal() +

    # Improve legend position
    theme(legend.position = "bottom")

  # Return the final ggplot object
  return(p)
}
\end{CodeInput}
```

### Running the project

The structure of the project should look like this:

```bash
.
├── functions
│   ├── functions.jl
│   ├── functions.py
│   └── functions.R
├── gen-env.R
└── gen-pipeline.R


2 directories, 5 files
```

Assuming we are working on a system that has Nix installed, we can "drop" into
a temporary Nix shell with R and `{rix}` available:

```r
nix-shell --expr "$(curl -sl https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix)"
```

Once the shell is ready, start R (by simply typing `R`) and run `source("gen-env.R")`
to generate the adequate `default.nix` file. This is the environment that we can use
to work interactively with the code while developing, and in which the pipeline will
be executed. We can then leave the R session and the shell entirely (by typing `exit`)
and type `nix-shell` to drop into the environment as defined by the `default.nix`.
The pipeline can now be built using `source("gen-pipeline.R")`.
